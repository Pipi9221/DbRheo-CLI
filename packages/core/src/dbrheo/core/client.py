"""
DatabaseClient - ä¸»æ§åˆ¶å™¨
è´Ÿè´£ä¼šè¯ç®¡ç†å’Œé€’å½’é€»è¾‘ï¼Œå®Œå…¨å¯¹é½Gemini CLIçš„Clientè®¾è®¡
"""

import asyncio
from typing import AsyncIterator, Optional, List
from typing import List, Dict, Any
from ..types.core_types import PartListUnion, AbortSignal, Content
from ..config.base import DatabaseConfig
from .chat import DatabaseChat
from ..utils.debug_logger import DebugLogger, log_info
from .turn import DatabaseTurn
from .scheduler import DatabaseToolScheduler
from .token_statistics import TokenStatistics


class DatabaseClient:
    """
    æ•°æ®åº“Agentä¸»æ§åˆ¶å™¨
    - ä¼šè¯ç®¡ç†å’Œé€’å½’é€»è¾‘ï¼ˆsend_message_streamï¼‰
    - å†å²å‹ç¼©æ£€æŸ¥å’Œè§¦å‘ï¼ˆtry_compress_chatï¼‰
    - next_speakeråˆ¤æ–­åè°ƒï¼ˆcheck_next_speakerï¼‰
    - å·¥å…·è°ƒåº¦å™¨é›†æˆ
    - é…ç½®å’Œç¯å¢ƒç®¡ç†
    """
    
    def __init__(self, config: DatabaseConfig):
        self.config = config
        self.chat = DatabaseChat(config)
        # ä¿å­˜å·²å®Œæˆçš„å·¥å…·è°ƒç”¨
        self.completed_tool_calls = []
        
        # åˆ›å»ºå·¥å…·æ³¨å†Œè¡¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼Œæœ€å°ä¾µå…¥æ€§ï¼‰
        from ..tools.registry import DatabaseToolRegistry
        self.tool_registry = DatabaseToolRegistry(config)
        # å°† tool_registry è®¾ç½®åˆ° config ä¸­ä¾›å…¶ä»–ç»„ä»¶ä½¿ç”¨
        config.set_test_config('tool_registry', self.tool_registry)
        
        # åˆ›å»ºè°ƒåº¦å™¨æ—¶è®¾ç½®å›è°ƒ
        self.tool_scheduler = DatabaseToolScheduler(
            config,
            on_all_tools_complete=self._on_tools_complete
        )
        # å°† tool_registry ä¼ é€’ç»™è°ƒåº¦å™¨ï¼ˆæœ€å°ä¾µå…¥æ€§ï¼‰
        self.tool_scheduler.tool_registry = self.tool_registry
        
        self.session_turn_count = 0
        self.token_statistics = TokenStatistics()  # Token ä½¿ç”¨ç»Ÿè®¡
        # ç¼“å­˜çš„JSONç”ŸæˆæœåŠ¡ï¼ˆæœ€å°ä¾µå…¥æ€§ä¼˜åŒ–ï¼‰
        self._json_llm_service = None
        
    def _on_tools_complete(self, completed_calls):
        """å·¥å…·æ‰§è¡Œå®Œæˆçš„å›è°ƒå¤„ç†"""
        # ä¿å­˜å·²å®Œæˆçš„å·¥å…·è°ƒç”¨
        self.completed_tool_calls = completed_calls
        if DebugLogger.should_log("DEBUG"):
            log_info("Client", f"Received {len(completed_calls)} completed tool calls from scheduler")
        
        # ä¸è¦åœ¨è¿™é‡Œå¤„ç†ï¼è®©ä¸»æµç¨‹æ¥å¤„ç†
        # å¦åˆ™ä¼šå¯¼è‡´ç­‰å¾…å¾ªç¯çœ‹ä¸åˆ°completed_tool_calls
        
    def _process_completed_tools(self):
        """
        å¤„ç†å·²å®Œæˆçš„å·¥å…·è°ƒç”¨ï¼Œå°†function responseæ·»åŠ åˆ°å†å²
        
        è¿™ä¸ªæ–¹æ³•è¢«è®¾è®¡ä¸ºå¯ä»¥å¤šæ¬¡è°ƒç”¨è€Œä¸ä¼šé‡å¤å¤„ç†ï¼š
        - ä»self.completed_tool_callsè·å–å¾…å¤„ç†çš„å·¥å…·
        - å¤„ç†åç«‹å³æ¸…ç©ºself.completed_tool_calls
        - è¿™æ ·å³ä½¿åœ¨ç¡®è®¤æµç¨‹ä¸­ä¹Ÿèƒ½ç¡®ä¿function responseè¢«æ­£ç¡®æ·»åŠ 
        
        è§£å†³çš„é—®é¢˜ï¼š
        1. ç¡®è®¤æµç¨‹å¯¼è‡´çš„function responseä¸¢å¤±
        2. ESCç»ˆæ­¢å¯èƒ½å¯¼è‡´çš„function responseæœªå¤„ç†
        3. ä»»ä½•å…¶ä»–å¯¼è‡´æ­£å¸¸æµç¨‹è¢«ä¸­æ–­çš„æƒ…å†µ
        """
        # è·å–å¾…å¤„ç†çš„å·¥å…·è°ƒç”¨
        completed_tools = self.completed_tool_calls
        if not completed_tools:
            return  # æ²¡æœ‰å¾…å¤„ç†çš„å·¥å…·
            
        # ç«‹å³æ¸…ç©ºï¼Œé¿å…é‡å¤å¤„ç†
        self.completed_tool_calls = []
        
        if DebugLogger.should_log("DEBUG"):
            log_info("Client", f"_process_completed_toolså¤„ç† {len(completed_tools)} ä¸ªå·¥å…·")
        
        # æ”¶é›†å·¥å…·å“åº”
        function_responses = []
        for tool_call in completed_tools:
            if hasattr(tool_call, 'response') and tool_call.response:
                # response_partså·²ç»æ˜¯functionResponseæ ¼å¼
                # é‡è¦ï¼šè¿›è¡Œæ·±åº¦å…‹éš†ï¼Œç¡®ä¿æ²¡æœ‰ protobuf å¯¹è±¡
                cloned_response = self.chat._safe_clone(tool_call.response.response_parts)
                function_responses.append(cloned_response)
                # åœ¨VERBOSEæ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†çš„functionResponse
                if DebugLogger.should_log("DEBUG") and DebugLogger.get_rules()["show_raw_chunks"]:
                    log_info("Client", f"Tool response: {tool_call.response.response_parts}")
                    
        if function_responses:
            if DebugLogger.should_log("DEBUG"):
                log_info("Client", f"æ”¶é›†åˆ° {len(function_responses)} ä¸ªå·¥å…·å“åº”")
            
            # å°†å·¥å…·å“åº”æ·»åŠ åˆ°å†å²è®°å½•
            # Gemini APIè¦æ±‚functionå“åº”ä½¿ç”¨'user'è§’è‰²
            function_content = {
                'role': 'user',
                'parts': function_responses
            }
            self.chat.add_history(function_content)
            
            if DebugLogger.should_log("DEBUG"):
                log_info("Client", f"function_contentå·²æ·»åŠ åˆ°å†å²ï¼ˆé€šè¿‡_process_completed_toolsï¼‰")
            
            DebugLogger.log_client_event("history_update", len(self.chat.get_history()))
            
            # åœ¨VERBOSEæ¨¡å¼ä¸‹æ˜¾ç¤ºæœ€æ–°çš„å†å²è®°å½•
            if DebugLogger.should_log("DEBUG") and DebugLogger.get_rules()["show_raw_chunks"]:
                history = self.chat.get_history()
                if history:
                    log_info("Client", f"Latest history entry: {history[-1]}")
                    
    async def send_message_stream(
        self, 
        request: PartListUnion, 
        signal: AbortSignal,
        prompt_id: str,
        turns: int = 100,
        original_model: Optional[str] = None
    ) -> AsyncIterator[dict]:
        """
        æ ¸å¿ƒé€’å½’é€»è¾‘ - ä¸Gemini CLIå®Œå…¨ä¸€è‡´
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å›æµå¼å“åº”
        """
        # åœ¨å¼€å§‹æ–°çš„æ¶ˆæ¯æµä¹‹å‰ï¼Œç¡®ä¿ä¹‹å‰çš„function responseè¢«å¤„ç†
        # è¿™è§£å†³äº†ç¡®è®¤æµç¨‹å’ŒESCç»ˆæ­¢å¯èƒ½å¯¼è‡´çš„function responseä¸¢å¤±é—®é¢˜
        self._process_completed_tools()
        
        self.session_turn_count += 1
        
        # 1. ä¼šè¯çº§åˆ«é™åˆ¶æ£€æŸ¥
        max_session_turns = self.config.get("max_session_turns", 50)
        if max_session_turns > 0 and self.session_turn_count > max_session_turns:
            yield {"type": "max_session_turns"}
            return
            
        # ç¡®ä¿turnsä¸è¶…è¿‡æœ€å¤§å€¼ï¼Œé˜²æ­¢æ— é™å¾ªç¯
        bounded_turns = min(turns, 100)
        if not bounded_turns:
            return
            
        # è·Ÿè¸ªåŸå§‹æ¨¡å‹ï¼Œæ£€æµ‹æ¨¡å‹åˆ‡æ¢
        initial_model = original_model or self.config.get_model()
        
        # 2. å†å²å‹ç¼©æ£€æŸ¥ï¼ˆæ•°æ®åº“Agentç®€åŒ–ç‰ˆï¼‰
        compressed = await self.try_compress_chat(prompt_id)
        if compressed:
            yield {"type": "chat_compressed", "value": compressed}
            
        # 3. æ‰§è¡Œå½“å‰Turnï¼ˆåªæ”¶é›†å·¥å…·è°ƒç”¨ï¼‰
        turn = DatabaseTurn(self.chat, prompt_id)
        async for event in turn.run(request, signal):
            # æ‹¦æˆª TokenUsage äº‹ä»¶è¿›è¡Œç»Ÿè®¡
            if event.get('type') == 'TokenUsage':
                # è¯¦ç»†è°ƒè¯•
                from ..utils.debug_logger import log_info
                log_info("Client", f"ğŸ“Š TOKEN STATISTICS - Adding usage to statistics:")
                log_info("Client", f"   - Turn count: {self.session_turn_count}")
                log_info("Client", f"   - Prompt ID: {prompt_id}")
                log_info("Client", f"   - prompt_tokens: {event['value'].get('prompt_tokens', 0)}")
                log_info("Client", f"   - completion_tokens: {event['value'].get('completion_tokens', 0)}")
                log_info("Client", f"   - total_tokens: {event['value'].get('total_tokens', 0)}")
                
                current_model = self.config.get_model() or "gemini-2.5-flash"
                self.token_statistics.add_usage(current_model, event['value'])
                # ä¸å‘ä¸Šä¼ é€’ TokenUsage äº‹ä»¶ï¼Œä¿æŒå‘åå…¼å®¹
            else:
                yield event
            
        # 4. å·¥å…·æ‰§è¡Œï¼ˆå¦‚æœæœ‰å¾…æ‰§è¡Œçš„å·¥å…·ï¼‰
        if turn.pending_tool_calls:
            DebugLogger.log_client_event("tools_found", len(turn.pending_tool_calls))
            
            # æ‰§è¡Œå·¥å…·ï¼ˆå¼‚æ­¥ï¼Œä¸ç­‰å¾…å®Œæˆï¼‰
            await self.tool_scheduler.schedule(turn.pending_tool_calls, signal)
            
            # å·¥å…·æ‰§è¡Œæ˜¯å¼‚æ­¥çš„ï¼Œè¿™é‡Œåªæ˜¯å¯åŠ¨äº†æ‰§è¡Œ
            # çœŸæ­£çš„å®Œæˆå¤„ç†åœ¨ _on_tools_complete å›è°ƒä¸­
            # ä¸ºäº†ç¡®ä¿å·¥å…·æ‰§è¡Œå®Œæˆï¼Œæˆ‘ä»¬éœ€è¦ç­‰å¾…å›è°ƒè¢«è§¦å‘
            
            # ç­‰å¾…å·¥å…·æ‰§è¡Œå®Œæˆï¼ˆé€šè¿‡æ£€æŸ¥ completed_tool_callsï¼‰
            # é‡è¦ï¼šå¦‚æœæœ‰å·¥å…·åœ¨ç­‰å¾…ç¡®è®¤ï¼Œä¸åº”è¯¥é˜»å¡ç­‰å¾…
            import asyncio
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·åœ¨ç­‰å¾…ç¡®è®¤
            has_awaiting_approval = any(
                call.status == 'awaiting_approval' 
                for call in self.tool_scheduler.tool_calls
            )
            
            if has_awaiting_approval:
                # æœ‰å·¥å…·ç­‰å¾…ç¡®è®¤ï¼Œç«‹å³è¿”å›è®©ç”¨æˆ·å¯ä»¥è¾“å…¥
                log_info("Client", "å·¥å…·ç­‰å¾…ç¡®è®¤ä¸­ï¼Œè¿”å›ç”¨æˆ·ç•Œé¢")
                # ç”Ÿæˆä¸€ä¸ªç­‰å¾…ç¡®è®¤çš„äº‹ä»¶
                yield {
                    "type": "AwaitingConfirmation",
                    "value": "æœ‰æ“ä½œéœ€è¦ç¡®è®¤ï¼Œè¯·æŸ¥çœ‹ä¸Šæ–¹æç¤º"
                }
                return  # ç»“æŸè¿™æ¬¡æ¶ˆæ¯æµï¼Œè®©ç”¨æˆ·å¯ä»¥è¾“å…¥ç¡®è®¤å‘½ä»¤
            
            # æ²¡æœ‰ç­‰å¾…ç¡®è®¤çš„å·¥å…·ï¼Œæ­£å¸¸ç­‰å¾…æ‰§è¡Œå®Œæˆ
            max_wait = 30  # æœ€å¤šç­‰å¾…30ç§’
            poll_interval = 0.1
            waited = 0
            
            while waited < max_wait and not self.completed_tool_calls:
                await asyncio.sleep(poll_interval)
                waited += poll_interval
                
            if waited >= max_wait and not self.completed_tool_calls:
                log_info("Client", f"Warning: Waited {max_wait}s but no tools completed")
            
            DebugLogger.log_client_event("execution_complete", {"count": len(self.tool_scheduler.tool_calls)})
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·²å®Œæˆçš„å·¥å…·éœ€è¦å¤„ç†
            # å…ˆä¿å­˜å¼•ç”¨ï¼Œé¿å…è¢«æ¸…ç©º
            has_completed_tools = len(self.completed_tool_calls) > 0
            
            # å°è¯•å¤„ç†å·²å®Œæˆçš„å·¥å…·ï¼ˆæ–°å¢çš„çµæ´»å¤„ç†æœºåˆ¶ï¼‰
            # è¿™ç¡®ä¿å³ä½¿åœ¨ç­‰å¾…æœŸé—´æœ‰æ–°çš„å·¥å…·å®Œæˆï¼Œä¹Ÿä¼šè¢«å¤„ç†
            self._process_completed_tools()
            
            # å¦‚æœ_process_completed_toolså·²ç»å¤„ç†äº†å·¥å…·ï¼Œæˆ‘ä»¬éœ€è¦ç»§ç»­å¯¹è¯
            if has_completed_tools:
                log_info("Client", "å·¥å…·å·²é€šè¿‡_process_completed_toolså¤„ç†ï¼Œå‘é€Please continue")
                DebugLogger.log_client_event("recursion_start", None)
                
                # æŒ‰ç…§è®¾è®¡æ–‡æ¡£ï¼Œå·¥å…·æ‰§è¡Œååº”è¯¥æ·»åŠ  "Please continue." è®©æ¨¡å‹ç»§ç»­
                async for event in self.send_message_stream(
                    [{"text": "Please continue."}],
                    signal,
                    prompt_id,
                    bounded_turns - 1,
                    initial_model
                ):
                    yield event
                return
            
            # åŸæœ‰é€»è¾‘ä¿æŒä¸å˜ï¼ˆä½œä¸ºåå¤‡æœºåˆ¶ï¼‰
            # è¿™ä¸ªåˆ†æ”¯ç†è®ºä¸Šä¸åº”è¯¥è¢«æ‰§è¡Œåˆ°ï¼Œå› ä¸º_process_completed_toolså·²ç»å¤„ç†äº†
            completed_tools = self.completed_tool_calls
            # æ¸…ç©ºä¿å­˜çš„è°ƒç”¨ï¼Œé¿å…é‡å¤å¤„ç†
            self.completed_tool_calls = []
            
            # ç²¾ç®€å·¥å…·æ‰§è¡Œæ—¥å¿—
            if DebugLogger.should_log("DEBUG"):
                for tool_call in completed_tools:
                    DebugLogger.log_scheduler_event("tool_complete", {"name": tool_call.request.name, "status": tool_call.status})
            
            # æ”¶é›†å·¥å…·å“åº”å¹¶æ·»åŠ åˆ°å†å²ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            if completed_tools:
                function_responses = []
                for tool_call in completed_tools:
                    if hasattr(tool_call, 'response') and tool_call.response:
                        # response_partså·²ç»æ˜¯functionResponseæ ¼å¼
                        # é‡è¦ï¼šè¿›è¡Œæ·±åº¦å…‹éš†ï¼Œç¡®ä¿æ²¡æœ‰ protobuf å¯¹è±¡
                        cloned_response = self.chat._safe_clone(tool_call.response.response_parts)
                        function_responses.append(cloned_response)
                        # åœ¨VERBOSEæ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†çš„functionResponse
                        if DebugLogger.should_log("DEBUG") and DebugLogger.get_rules()["show_raw_chunks"]:
                            log_info("Client", f"Tool response: {tool_call.response.response_parts}")
                        
                if function_responses:
                    if DebugLogger.should_log("DEBUG"):
                        log_info("Client", f"æ”¶é›†åˆ° {len(function_responses)} ä¸ªå·¥å…·å“åº”")
                    
                    # å°†å·¥å…·å“åº”æ·»åŠ åˆ°å†å²è®°å½•
                    # Gemini APIè¦æ±‚functionå“åº”ä½¿ç”¨'user'è§’è‰²
                    function_content = {
                        'role': 'user',
                        'parts': function_responses
                    }
                    self.chat.add_history(function_content)
                    
                    DebugLogger.log_client_event("history_update", len(self.chat.get_history()))
                    
                    # åœ¨VERBOSEæ¨¡å¼ä¸‹æ˜¾ç¤ºæœ€æ–°çš„å†å²è®°å½•
                    if DebugLogger.should_log("DEBUG") and DebugLogger.get_rules()["show_raw_chunks"]:
                        history = self.chat.get_history()
                        if history:
                            log_info("Client", f"Latest history entry: {history[-1]}")
                    
                    # è°ƒåº¦å™¨ä¼šè‡ªåŠ¨åœ¨ _check_and_notify_completion ä¸­æ¸…ç†çŠ¶æ€
                    # ä¸éœ€è¦æ‰‹åŠ¨æ¸…ç† self.tool_scheduler.tool_calls
                    
                    DebugLogger.log_client_event("recursion_start", None)
                    
                    log_info("Client", f"ğŸ”„ RECURSION #2 - After collecting tool responses")
                    # æŒ‰ç…§è®¾è®¡æ–‡æ¡£ï¼Œå·¥å…·æ‰§è¡Œååº”è¯¥æ·»åŠ  "Please continue." è®©æ¨¡å‹ç»§ç»­
                    # è¿™ç¬¦åˆ Gemini CLI çš„è®¾è®¡æ¨¡å¼
                    async for event in self.send_message_stream(
                        [{"text": "Please continue."}],  # æ·»åŠ  Please continue.
                        signal,
                        prompt_id,
                        bounded_turns - 1,
                        initial_model
                    ):
                        yield event
                    return
            
        # 5. é€’å½’å†³ç­–ï¼ˆåªåœ¨æ²¡æœ‰å¾…æ‰§è¡Œå·¥å…·ä¸”æœªä¸­æ­¢æ—¶åˆ¤æ–­ï¼‰
        if not turn.pending_tool_calls and signal and not signal.aborted:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¢«åˆ‡æ¢ï¼ˆé˜²æ­¢é™çº§åçš„æ„å¤–é€’å½’ï¼‰
            current_model = self.config.get_model()
            if current_model != initial_model:
                return
                
            # AIè‡ªä¸»åˆ¤æ–­ä¸‹ä¸€æ­¥
            from .next_speaker import check_next_speaker
            next_speaker_check = await check_next_speaker(self.chat, self, signal)
            if next_speaker_check and next_speaker_check.get('next_speaker') == 'model':
                # é€’å½’è°ƒç”¨ï¼šæ·»åŠ "Please continue."å¹¶ç»§ç»­
                next_request = [{'text': 'Please continue.'}]
                async for event in self.send_message_stream(
                    next_request,
                    signal,
                    prompt_id,
                    bounded_turns - 1,
                    initial_model
                ):
                    yield event
        
    async def try_compress_chat(self, prompt_id: str, force: bool = False):
        """
        å†å²å‹ç¼© - æ•°æ®åº“Agentç®€åŒ–ç‰ˆæœ¬
        æ•°æ®åº“Agenté€šå¸¸ä¸éœ€è¦å†å²å‹ç¼©ï¼Œè¿”å›Noneå³å¯
        """
        # æ•°æ®åº“Agentç‰¹æ€§ï¼šä¸éœ€è¦å¤æ‚çš„å†å²å‹ç¼©
        # åŸå› ï¼š
        # 1. ä»»åŠ¡é©±åŠ¨çš„å¯¹è¯æ¨¡å¼ï¼Œä¸æ˜¯é•¿æ—¶é—´è¿ç»­å¯¹è¯
        # 2. æ•°æ®åº“ç»“æ„ä¿¡æ¯ç›¸å¯¹å›ºå®šï¼Œä¸éœ€è¦å‹ç¼©
        # 3. æ“ä½œé€šå¸¸æ˜¯çŸ­æœŸä»»åŠ¡ï¼Œä¸å¤ªå¯èƒ½è§¦å‘tokené™åˆ¶
        return None
        
    async def generate_json(
        self,
        contents: List[Content],
        schema: Dict[str, Any],
        signal: AbortSignal,
        system_instruction: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ç”ŸæˆJSONå“åº” - ç”¨äºnext_speakeråˆ¤æ–­ç­‰ç»“æ„åŒ–è¾“å‡º
        """
        from ..services.llm_factory import create_llm_service
        
        # ä½¿ç”¨ç¼“å­˜çš„LLMæœåŠ¡ï¼ˆæœ€å°ä¾µå…¥æ€§ä¼˜åŒ–ï¼‰
        if self._json_llm_service is None:
            log_info("Client", "Creating JSON LLM service (first time only)")
            self._json_llm_service = create_llm_service(self.config)
        
        # è°ƒç”¨æœåŠ¡ç”ŸæˆJSON
        return await self._json_llm_service.generate_json(
            contents,
            schema,
            signal,
            system_instruction
        )
