"""
next_speakeråˆ¤æ–­é€»è¾‘ - å®Œå…¨å‚è€ƒGemini CLIçš„checkNextSpeaker
AIè‡ªä¸»åˆ¤æ–­ä¸‹ä¸€æ­¥æ˜¯ç»§ç»­æ‰§è¡Œè¿˜æ˜¯ç­‰å¾…ç”¨æˆ·è¾“å…¥
"""

from ..utils.content_helper import get_parts, get_role, get_text
from typing import Optional, Dict, Any
from ..types.core_types import AbortSignal
from .chat import DatabaseChat
from .prompts import DatabasePromptManager
from ..utils.debug_logger import log_info, DebugLogger


# JSON Schemaå®šä¹‰
NEXT_SPEAKER_SCHEMA = {
    "type": "object",
    "properties": {
        "next_speaker": {
            "type": "string",
            "enum": ["user", "model"],
            "description": "Who should speak next"
        },
        "reasoning": {
            "type": "string",
            "description": "Explanation for the decision"
        }
    },
    "required": ["next_speaker", "reasoning"]
}


async def check_next_speaker(
    chat: DatabaseChat, 
    client: 'DatabaseClient', 
    signal: AbortSignal
) -> Optional[Dict[str, Any]]:
    """
    AIè‡ªä¸»åˆ¤æ–­ä¸‹ä¸€æ­¥ - ä¸Gemini CLIçš„checkNextSpeakerå®Œå…¨ä¸€è‡´
    
    åˆ¤æ–­è§„åˆ™ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
    1. ç‰¹æ®Šæƒ…å†µä¼˜å…ˆå¤„ç†ï¼š
       - æœ€åæ˜¯å·¥å…·æ‰§è¡Œç»“æœ â†’ modelç»§ç»­å¤„ç†ç»“æœ
       - æœ€åæ˜¯ç©ºçš„modelæ¶ˆæ¯ â†’ modelç»§ç»­å®Œæˆå“åº”
    2. AIæ™ºèƒ½åˆ¤æ–­ï¼ˆé€šè¿‡ä¸´æ—¶æç¤ºè¯è¯¢é—®ï¼‰ï¼š
       - Modelç»§ç»­ï¼šæ˜ç¡®è¡¨ç¤ºä¸‹ä¸€æ­¥åŠ¨ä½œ
       - Userå›ç­”ï¼šå‘ç”¨æˆ·æå‡ºäº†éœ€è¦å›ç­”çš„é—®é¢˜
       - Userè¾“å…¥ï¼šå®Œæˆå½“å‰ä»»åŠ¡ï¼Œç­‰å¾…æ–°æŒ‡ä»¤
    """
    # è°ƒè¯•
    log_info("NextSpeaker", f"ğŸ¤” CHECK_NEXT_SPEAKER called")
    
    # 1. ç‰¹æ®Šæƒ…å†µä¼˜å…ˆå¤„ç†ï¼ˆä¸Gemini CLIé€»è¾‘ä¸€è‡´ï¼‰
    curated_history = chat.get_history(True)
    if not curated_history:
        return None
        
    last_message = curated_history[-1]
    
    # å·¥å…·åˆšæ‰§è¡Œå®Œï¼ŒAIåº”è¯¥ç»§ç»­å¤„ç†ç»“æœ
    if get_role(last_message) == 'function':
        return {
            'next_speaker': 'model',
            'reasoning': 'Function response received, model should process the result'
        }
        
    # ç©ºçš„modelæ¶ˆæ¯ï¼Œåº”è¯¥ç»§ç»­å®Œæˆå“åº”
    if (get_role(last_message) == 'model' and
        not any(get_text(part).strip() for part in get_parts(last_message))):
        return {
            'next_speaker': 'model',
            'reasoning': 'Empty model response, should continue'
        }
        
    # 2. AIæ™ºèƒ½åˆ¤æ–­ï¼ˆä¸´æ—¶æç¤ºè¯ï¼Œä¸ä¿å­˜åˆ°å†å²ï¼‰
    prompt_manager = DatabasePromptManager()
    check_prompt = prompt_manager.get_next_speaker_prompt()
    
    # æ„å»ºä¸´æ—¶å†…å®¹ï¼ˆä¸Gemini CLIæ–¹å¼ä¸€è‡´ï¼‰
    contents = [
        *curated_history,
        {'role': 'user', 'parts': [{'text': check_prompt}]}
    ]
    
    # 3. è°ƒç”¨LLMåˆ¤æ–­ï¼ˆä½¿ç”¨ç›¸åŒçš„æ¨¡å‹å’Œé…ç½®ï¼‰
    try:
        response = await client.generate_json(
            contents,
            NEXT_SPEAKER_SCHEMA,
            signal,
            # ä½¿ç”¨ä¸´æ—¶ç³»ç»ŸæŒ‡ä»¤è¦†ç›–ï¼ˆä¸å½±å“ä¸»å¯¹è¯ï¼‰
            system_instruction=""  # æ¸…ç©ºç³»ç»ŸæŒ‡ä»¤ï¼Œä¸“æ³¨åˆ¤æ–­ä»»åŠ¡
        )
        return response
    except Exception as e:
        # åˆ¤æ–­å¤±è´¥æ—¶çš„é»˜è®¤è¡Œä¸º
        return {
            'next_speaker': 'user',
            'reasoning': f'Failed to determine next speaker: {str(e)}'
        }
