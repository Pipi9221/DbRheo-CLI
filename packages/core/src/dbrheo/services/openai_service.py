"""
OpenAI API æœåŠ¡ - å¤„ç†ä¸ OpenAI API çš„é€šä¿¡
æ”¯æŒ GPT-3.5ã€GPT-4ã€o1 ç­‰æ¨¡å‹ï¼Œä¿æŒä¸ GeminiService ç›¸åŒçš„æ¥å£
"""

from ..utils.content_helper import get_parts, get_role, get_text
import os
import json
from typing import List, Dict, Any, Optional, Iterator
from ..types.core_types import Content, AbortSignal
from ..config.base import DatabaseConfig
from ..utils.debug_logger import DebugLogger, log_info, log_error
from ..utils.retry_with_backoff import retry_with_backoff_sync, RetryOptions


class OpenAIService:
    """
    OpenAI API æœåŠ¡
    - ä¸ OpenAI API çš„é€šä¿¡
    - æ¶ˆæ¯æ ¼å¼è½¬æ¢ï¼ˆGemini â†” OpenAIï¼‰
    - å‡½æ•°è°ƒç”¨å¤„ç†
    - æµå¼å“åº”å¤„ç†
    """
    
    def __init__(self, config: DatabaseConfig):
        self.config = config
        self._setup_api()
        
    def _setup_api(self):
        """è®¾ç½® OpenAI API"""
        # è·å– API å¯†é’¥ - æ”¯æŒå¤šç§é…ç½®æ–¹å¼ï¼ˆåŒ…æ‹¬é˜¿é‡Œç™¾ç‚¼ï¼‰
        api_key = (
            self.config.get("openai_api_key") or
            os.getenv("OPENAI_API_KEY") or
            self.config.get("dashscope_api_key") or
            os.getenv("DASHSCOPE_API_KEY") or
            self.config.get("ali_bailian_api_key") or
            os.getenv("ALI_BAILIAN_API_KEY")
        )
        
        if not api_key:
            raise ValueError("OPENAI_API_KEY or DASHSCOPE_API_KEY environment variable is required")
            
        # æ”¯æŒè‡ªå®šä¹‰ API åŸºç¡€ URLï¼ˆå…¼å®¹ APIï¼ŒåŒ…æ‹¬é˜¿é‡Œç™¾ç‚¼ï¼‰
        api_base = (
            self.config.get("openai_api_base") or
            os.getenv("OPENAI_API_BASE") or
            self.config.get("dashscope_base_url") or
            os.getenv("DASHSCOPE_BASE_URL") or
            self.config.get("ali_bailian_api_base") or
            os.getenv("ALI_BAILIAN_API_BASE") or
            "https://api.openai.com/v1"
        )
        
        # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…æœªå®‰è£…æ—¶æŠ¥é”™
        try:
            import openai
        except ImportError:
            raise ImportError(
                "openai package is not installed. "
                "Please install it with: pip install openai>=1.0"
            )
            
        self.client = openai.OpenAI(
            api_key=api_key,
            base_url=api_base
        )
        
        # é…ç½®æ¨¡å‹ - æ”¯æŒå¤šç§ OpenAI æ¨¡å‹
        model_name = self.config.get_model()
        # æ˜ å°„ç®€çŸ­åç§°åˆ°å®Œæ•´æ¨¡å‹åï¼ˆåªä¿ç•™æ ¸å¿ƒæ¨¡å‹ï¼‰
        model_mappings = {
            # é»˜è®¤åˆ«å
            "gpt": "gpt-4.1",  # é»˜è®¤ä½¿ç”¨ GPT-4.1
            "openai": "gpt-4.1",
            
            # GPT-4.1 ç³»åˆ— (2025å¹´4æœˆå‘å¸ƒ)
            "gpt-4.1": "gpt-4.1",
            "gpt4.1": "gpt-4.1",
            
            # GPT-5 Mini (2025å¹´8æœˆå‘å¸ƒ)
            "gpt-mini": "gpt-5-mini",
            "gpt-5-mini": "gpt-5-mini",
            "mini": "gpt-5-mini"
        }
        
        # å¦‚æœæ˜¯ç®€çŸ­åç§°ï¼Œè½¬æ¢ä¸ºå®Œæ•´åç§°
        for short_name, full_name in model_mappings.items():
            if model_name.lower().startswith(short_name):
                self.model_name = full_name
                break
        else:
            # ä½¿ç”¨åŸå§‹åç§°
            self.model_name = model_name
            
        log_info("OpenAI", f"Using model: {self.model_name}")
        
        # é»˜è®¤ç”Ÿæˆé…ç½®
        self.default_generation_config = {
            "temperature": 0.7,
            "max_tokens": 8192,
            "top_p": 0.8,
        }
        
    def send_message_stream(
        self,
        contents: List[Content],
        tools: Optional[List[Dict[str, Any]]] = None,
        system_instruction: Optional[str] = None,
        signal: Optional[AbortSignal] = None
    ) -> Iterator[Dict[str, Any]]:
        """
        å‘é€æ¶ˆæ¯å¹¶è¿”å›æµå¼å“åº”ï¼ˆåŒæ­¥ç”Ÿæˆå™¨ï¼‰
        ä¿æŒä¸ GeminiService ç›¸åŒçš„æ¥å£
        """
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            messages = self._gemini_to_openai_messages(contents, system_instruction)
            
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            request_params = {
                "model": self.model_name,
                "messages": messages,
                "stream": True,
                "stream_options": {"include_usage": True},  # å¯ç”¨æµå¼å“åº”ä¸­çš„ token ç»Ÿè®¡
                **self.default_generation_config
            }
            
            # å¤„ç†å‡½æ•°è°ƒç”¨
            if tools:
                openai_tools = self._convert_tools_to_openai_format(tools)
                if openai_tools:
                    request_params["tools"] = openai_tools
                    request_params["tool_choice"] = "auto"
            
            # ä½¿ç”¨é‡è¯•æœºåˆ¶
            def api_call():
                return self.client.chat.completions.create(**request_params)
                
            retry_options = RetryOptions(
                max_attempts=3,
                initial_delay_ms=2000,
                max_delay_ms=10000
            )
            
            stream = retry_with_backoff_sync(api_call, retry_options)
            
            # å¤„ç†æµå¼å“åº”
            chunk_count = 0
            current_function_call = None
            
            for chunk in stream:
                chunk_count += 1
                
                # è°ƒè¯•ï¼šæ£€æŸ¥æ¯ä¸ª chunk çš„ç»“æ„
                if DebugLogger.should_log("DEBUG"):
                    log_info("OpenAI", f"Chunk #{chunk_count}: has_usage={hasattr(chunk, 'usage')}, has_choices={bool(chunk.choices)}")
                
                if signal and signal.aborted:
                    break
                    
                # å…ˆè·Ÿè¸ªå‡½æ•°è°ƒç”¨çŠ¶æ€
                if chunk.choices and chunk.choices[0].delta.tool_calls:
                    for tool_call in chunk.choices[0].delta.tool_calls:
                        if tool_call.function:
                            if not current_function_call:
                                current_function_call = {
                                    "id": tool_call.id or f"call_{chunk_count}",
                                    "name": tool_call.function.name or "",
                                    "arguments": ""
                                }
                            if tool_call.function.arguments:
                                current_function_call["arguments"] += tool_call.function.arguments
                
                # ç„¶åå¤„ç† chunk
                processed = self._process_openai_chunk(chunk, current_function_call)
                
                if processed:
                    DebugLogger.log_gemini_chunk(chunk_count, chunk, processed)
                    yield processed
                    
                    # å¦‚æœå·²ç»ç”Ÿæˆäº†å‡½æ•°è°ƒç”¨ï¼Œé‡ç½®çŠ¶æ€
                    if processed.get("function_calls"):
                        current_function_call = None
            
            # æµç»“æŸåï¼Œå¦‚æœè¿˜æœ‰æœªè¿”å›çš„å‡½æ•°è°ƒç”¨ï¼Œç«‹å³è¿”å›
            if current_function_call and current_function_call.get("name") and current_function_call.get("arguments"):
                try:
                    args = json.loads(current_function_call["arguments"])
                    log_info("OpenAI", f"âœ… Returning accumulated function call at stream end: {current_function_call['name']}")
                    yield {
                        "function_calls": [{
                            "id": current_function_call["id"],
                            "name": current_function_call["name"],
                            "args": args
                        }]
                    }
                except Exception as e:
                    log_error("OpenAI", f"Failed to parse accumulated function call: {e}")
                    
        except Exception as e:
            log_error("OpenAI", f"API error: {type(e).__name__}: {str(e)}")
            
            if DebugLogger.should_log("DEBUG"):
                error_message = f"OpenAI API error: {type(e).__name__}: {str(e)}"
            else:
                error_message = "OpenAI API is temporarily unavailable. Please try again."
                
            yield self._create_error_chunk(error_message)
            
    async def generate_json(
        self,
        contents: List[Content],
        schema: Dict[str, Any],
        signal: Optional[AbortSignal] = None,
        system_instruction: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆ JSON å“åº” - ä½¿ç”¨ OpenAI çš„ JSON æ¨¡å¼
        """
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            messages = self._gemini_to_openai_messages(contents, system_instruction)
            
            # ç§»é™¤æ‰€æœ‰ tool ç›¸å…³æ¶ˆæ¯ï¼ˆJSON ç”Ÿæˆä¸éœ€è¦å·¥å…·è°ƒç”¨ï¼‰
            messages = [msg for msg in messages if msg["role"] not in ["tool"] and "tool_calls" not in msg]
            
            # æ·»åŠ  JSON æŒ‡ä»¤
            json_instruction = f"Respond with valid JSON matching this schema: {json.dumps(schema, indent=2)}"
            messages.append({"role": "user", "content": json_instruction})
            
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            request_params = {
                "model": self.model_name,
                "messages": messages,
                "temperature": 0.1,  # ä½æ¸©åº¦ä»¥æé«˜ä¸€è‡´æ€§
                "max_tokens": 4096,
                "response_format": {"type": "json_object"}  # JSON æ¨¡å¼
            }
            
            # åŒæ­¥è°ƒç”¨ï¼ˆæ³¨æ„ï¼šè¿™æ˜¯ async æ–¹æ³•ä½†ä½¿ç”¨åŒæ­¥ APIï¼‰
            import asyncio
            loop = asyncio.get_event_loop()
            
            def sync_call():
                response = self.client.chat.completions.create(**request_params)
                return response.choices[0].message.content
                
            response_text = await loop.run_in_executor(None, sync_call)
            
            # è§£æ JSON
            return json.loads(response_text)
                
        except Exception as e:
            log_error("OpenAI", f"JSON generation error: {str(e)}")
            # è¿”å›é»˜è®¤å“åº”
            return {
                "next_speaker": "user",
                "reasoning": f"Error in JSON generation: {str(e)}"
            }
            
    def _gemini_to_openai_messages(
        self, 
        contents: List[Content], 
        system_instruction: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        å°† Gemini æ ¼å¼çš„æ¶ˆæ¯è½¬æ¢ä¸º OpenAI æ ¼å¼
        Gemini: {"role": "user/model", "parts": [{"text": "..."}]}
        OpenAI: {"role": "user/assistant/system", "content": "..."}
        """
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        if system_instruction:
            messages.append({
                "role": "system",
                "content": system_instruction
            })
        
        # å…ˆæ”¶é›†æ‰€æœ‰æ¶ˆæ¯ï¼ŒåŒ…æ‹¬toolå“åº”
        tool_responses_pending = []  # å¾…å¤„ç†çš„toolå“åº”
        
        for content in contents:
            # è½¬æ¢è§’è‰²
            role = "assistant" if get_role(content) == "model" else content.get("role", "user")
            
            # æå–å†…å®¹
            text_parts = []
            tool_calls = []
            
            parts = get_parts(content)
            for part in parts:
                if isinstance(part, dict):
                    if "text" in part:
                        text_parts.append(part["text"])
                    elif "function_call" in part:
                        # è½¬æ¢å‡½æ•°è°ƒç”¨
                        fc = part["function_call"]
                        tool_calls.append({
                            "id": fc.get("id", f"call_{len(tool_calls)}"),
                            "type": "function",
                            "function": {
                                "name": fc.get("name", ""),
                                "arguments": json.dumps(fc.get("args", {}))
                            }
                        })
                    elif "function_response" in part or "functionResponse" in part:
                        # æ”¶é›†å‡½æ•°å“åº”ï¼Œç¨åå¤„ç†
                        fr = part.get("function_response") or part.get("functionResponse")
                        tool_responses_pending.append({
                            "role": "tool",
                            "tool_call_id": fr.get("id", ""),
                            "content": json.dumps(fr.get("response", {}))
                        })
            
            # æ„å»ºæ¶ˆæ¯
            if text_parts or tool_calls:
                message = {"role": role}
                
                if text_parts:
                    message["content"] = "\n".join(text_parts)
                else:
                    message["content"] = ""  # OpenAI è¦æ±‚ content å­—æ®µ
                    
                if tool_calls and role == "assistant":
                    message["tool_calls"] = tool_calls
                    
                messages.append(message)
        
        # å¤„ç†å‰©ä½™çš„toolå“åº”ï¼ˆå¦‚æœæœ‰ï¼‰
        if tool_responses_pending:
            messages.extend(tool_responses_pending)
        
        # ä¿®å¤tool_callså’Œtoolå“åº”çš„é…å¯¹é—®é¢˜
        # ç§»é™¤æ‰“æ–­é…å¯¹çš„ç”¨æˆ·æ¶ˆæ¯
        fixed_messages = []
        if system_instruction:
            # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
            fixed_messages.append(messages[0])
            start_idx = 1
        else:
            start_idx = 0
            
        # è°ƒè¯•ï¼šæ‰“å°ä¿®å¤å‰çš„æ¶ˆæ¯
        if DebugLogger.should_log("DEBUG"):
            log_info("OpenAI", f"ä¿®å¤å‰çš„æ¶ˆæ¯æ•°é‡: {len(messages)}")
            for idx, msg in enumerate(messages):
                role = get_role(msg)
                if "tool_calls" in msg:
                    log_info("OpenAI", f"  [{idx}] {role} - has tool_calls: {[tc['id'] for tc in msg['tool_calls']]}")
                elif role == "tool":
                    log_info("OpenAI", f"  [{idx}] {role} - tool_call_id: {msg.get('tool_call_id', 'none')}")
                else:
                    content_preview = str(msg.get("content", ""))[:50]
                    log_info("OpenAI", f"  [{idx}] {role} - {content_preview}")
            
        # å…ˆæ”¶é›†æ‰€æœ‰çš„toolå“åº”ï¼Œå»ºç«‹IDåˆ°å“åº”çš„æ˜ å°„
        tool_responses_map = {}
        for msg in messages[start_idx:]:
            if msg["role"] == "tool" and "tool_call_id" in msg:
                tool_responses_map[msg["tool_call_id"]] = msg
        
        # è®°å½•å·²ä½¿ç”¨çš„toolå“åº”
        used_tool_ids = set()
        
        i = start_idx
        while i < len(messages):
            msg = messages[i]
            # æ£€æŸ¥æ˜¯å¦æ˜¯åŒ…å«tool_callsçš„assistantæ¶ˆæ¯
            if msg["role"] == "assistant" and "tool_calls" in msg:
                # æ·»åŠ å½“å‰æ¶ˆæ¯
                fixed_messages.append(msg)
                
                # ç«‹å³æ·»åŠ å¯¹åº”çš„toolå“åº”ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                for tool_call in msg["tool_calls"]:
                    tool_id = tool_call["id"]
                    if tool_id in tool_responses_map and tool_id not in used_tool_ids:
                        fixed_messages.append(tool_responses_map[tool_id])
                        used_tool_ids.add(tool_id)
                
                i += 1
            elif msg["role"] == "tool":
                # è·³è¿‡å·²ç»æ·»åŠ çš„toolå“åº”
                if msg.get("tool_call_id") in used_tool_ids:
                    i += 1
                    continue
                else:
                    # å­¤ç«‹çš„toolå“åº”ï¼Œä¿ç•™å®ƒ
                    fixed_messages.append(msg)
                    i += 1
            else:
                # å…¶ä»–æ¶ˆæ¯ï¼šè·³è¿‡æ‰“æ–­toolé…å¯¹çš„"Please continue"
                if (msg["role"] == "user" and 
                    msg.get("content", "").strip() in ["Please continue.", "Continue the conversation."] and
                    i > start_idx and 
                    len(fixed_messages) > 0):
                    # æ£€æŸ¥å‰ä¸€æ¡æ¶ˆæ¯æ˜¯å¦æ˜¯æœªé…å¯¹çš„assistant with tool_calls
                    prev_msg = fixed_messages[-1]
                    if prev_msg["role"] == "assistant" and "tool_calls" in prev_msg:
                        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰tool_callséƒ½æœ‰å“åº”
                        all_paired = all(
                            tc["id"] in used_tool_ids 
                            for tc in prev_msg["tool_calls"]
                        )
                        if not all_paired:
                            # è·³è¿‡è¿™ä¸ª"Please continue"ï¼Œå› ä¸ºå®ƒæ‰“æ–­äº†é…å¯¹
                            i += 1
                            continue
                
                # æ·»åŠ å…¶ä»–æ­£å¸¸æ¶ˆæ¯
                fixed_messages.append(msg)
                i += 1
                
        # æ£€æŸ¥æ˜¯å¦æœ‰æœªé…å¯¹çš„tool_callsï¼Œä¸ºå®ƒä»¬ç”Ÿæˆå ä½å“åº”
        # è¿™è§£å†³äº†å·¥å…·ç­‰å¾…ç¡®è®¤æ—¶çš„é…å¯¹é—®é¢˜
        final_messages = []
        for i, msg in enumerate(fixed_messages):
            final_messages.append(msg)
            
            # å¦‚æœæ˜¯åŒ…å«tool_callsçš„assistantæ¶ˆæ¯
            if msg["role"] == "assistant" and "tool_calls" in msg:
                # æ£€æŸ¥æ¯ä¸ªtool_callæ˜¯å¦æœ‰å¯¹åº”çš„å“åº”
                for tool_call in msg["tool_calls"]:
                    tool_id = tool_call["id"]
                    # æ£€æŸ¥ä¸‹ä¸€æ¡æ¶ˆæ¯æ˜¯å¦æ˜¯å¯¹åº”çš„toolå“åº”
                    has_response = False
                    if i + 1 < len(fixed_messages):
                        next_msg = fixed_messages[i + 1]
                        if next_msg["role"] == "tool" and next_msg.get("tool_call_id") == tool_id:
                            has_response = True
                    
                    # å¦‚æœæ²¡æœ‰å“åº”ï¼Œç”Ÿæˆå ä½å“åº”
                    if not has_response:
                        placeholder_response = {
                            "role": "tool",
                            "tool_call_id": tool_id,
                            "content": "Tool execution pending or awaiting confirmation"
                        }
                        final_messages.append(placeholder_response)
                        # è®°å½•è¿™ä¸ªå ä½å“åº”
                        if DebugLogger.should_log("DEBUG"):
                            log_info("OpenAI", f"Generated placeholder response for tool_call_id: {tool_id}")
        
        # è°ƒè¯•ï¼šæ‰“å°ä¿®å¤åçš„æ¶ˆæ¯
        if DebugLogger.should_log("DEBUG"):
            log_info("OpenAI", f"ä¿®å¤åçš„æ¶ˆæ¯æ•°é‡: {len(final_messages)}")
            for idx, msg in enumerate(final_messages):
                role = get_role(msg)
                if "tool_calls" in msg:
                    log_info("OpenAI", f"  [{idx}] {role} - has tool_calls: {[tc['id'] for tc in msg['tool_calls']]}")
                elif role == "tool":
                    log_info("OpenAI", f"  [{idx}] {role} - tool_call_id: {msg.get('tool_call_id', 'none')}")
                else:
                    content_preview = str(msg.get("content", ""))[:50]
                    log_info("OpenAI", f"  [{idx}] {role} - {content_preview}")
                
        return final_messages
        
    def _convert_tools_to_openai_format(self, tools: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å°† Gemini å·¥å…·æ ¼å¼è½¬æ¢ä¸º OpenAI æ ¼å¼"""
        openai_tools = []
        
        for tool in tools:
            openai_tool = {
                "type": "function",
                "function": {
                    "name": tool.get("name", ""),
                    "description": tool.get("description", ""),
                    "parameters": tool.get("parameters", {})
                }
            }
            openai_tools.append(openai_tool)
            
        return openai_tools
        
    def _process_openai_chunk(
        self, 
        chunk, 
        current_function_call: Optional[Dict] = None
    ) -> Optional[Dict[str, Any]]:
        """å¤„ç† OpenAI æµå¼ chunkï¼Œè½¬æ¢ä¸º Gemini æ ¼å¼"""
        result = {}
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ usage ä¿¡æ¯ï¼ˆå¯èƒ½åœ¨æœ€åä¸€ä¸ªæ²¡æœ‰ choices çš„ chunk ä¸­ï¼‰
        if hasattr(chunk, 'usage') and chunk.usage:
            usage = chunk.usage
            token_info = {
                "prompt_tokens": getattr(usage, 'prompt_tokens', 0),
                "completion_tokens": getattr(usage, 'completion_tokens', 0),
                "total_tokens": getattr(usage, 'total_tokens', 0)
            }
            
            # OpenAIçš„cached_tokensåœ¨prompt_tokens_detailsä¸­
            cached_tokens = 0
            if hasattr(usage, 'prompt_tokens_details'):
                details = usage.prompt_tokens_details
                if hasattr(details, 'cached_tokens'):
                    cached_tokens = getattr(details, 'cached_tokens', 0)
                # è°ƒè¯•ï¼šæŸ¥çœ‹detailsçš„æ‰€æœ‰å±æ€§
                from ..utils.debug_logger import log_info
                if DebugLogger.should_log("DEBUG"):
                    attrs = [attr for attr in dir(details) if not attr.startswith('_')] if details else []
                    log_info("OpenAI", f"prompt_tokens_details attributes: {attrs}")
            
            token_info["cached_tokens"] = cached_tokens
            result["token_usage"] = token_info
            
            # è°ƒè¯•æ—¥å¿—
            log_info("OpenAI", f"Token usage detected: {token_info}")
            if cached_tokens > 0:
                log_info("OpenAI", f"Prompt caching active - Cached tokens: {cached_tokens}")
            # å¦‚æœåªæœ‰ usage ä¿¡æ¯ï¼Œç›´æ¥è¿”å›
            if not chunk.choices:
                return result
        
        if not chunk.choices:
            return None
            
        choice = chunk.choices[0]
        delta = choice.delta
        
        # å¤„ç†æ–‡æœ¬å†…å®¹
        if delta.content:
            result["text"] = delta.content
            
        # å¤„ç†å‡½æ•°è°ƒç”¨å®Œæˆ
        if choice.finish_reason == "tool_calls" and current_function_call:
            # è§£æå‚æ•°  
            try:
                args = json.loads(current_function_call["arguments"])
                from ..utils.debug_logger import log_info
                log_info("OpenAI", f"âœ… Function call parsed successfully:")
                log_info("OpenAI", f"  Function: {current_function_call.get('name', 'unknown')}")
                log_info("OpenAI", f"  Raw arguments: {repr(current_function_call.get('arguments', ''))}")
                log_info("OpenAI", f"  Parsed args: {repr(args)}")
            except Exception as e:
                from ..utils.debug_logger import log_info
                log_info("OpenAI", f"ğŸš¨ Failed to parse function arguments:")
                log_info("OpenAI", f"  Function: {current_function_call.get('name', 'unknown')}")
                log_info("OpenAI", f"  Raw arguments: {repr(current_function_call.get('arguments', ''))}")
                log_info("OpenAI", f"  Parse error: {e}")
                
                # å°è¯•ä»ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ä¸­æå–å‚æ•°
                args = self._extract_first_valid_json(current_function_call["arguments"])
                if args:
                    log_info("OpenAI", f"âœ… Recovered from malformed JSON: {repr(args)}")
                else:
                    log_info("OpenAI", f"âŒ Could not recover from malformed JSON")
                    args = {}
                
            result["function_calls"] = [{
                "id": current_function_call["id"],
                "name": current_function_call["name"],
                "args": args
            }]
            
        return result if result else None
    
    def _extract_first_valid_json(self, text: str) -> Dict[str, Any]:
        """
        æ™ºèƒ½å¤„ç†å¤šä¸ªJSONå¯¹è±¡çš„æƒ…å†µï¼Œä¿æŒAgentçµæ´»æ€§
        """
        if not text or not text.strip():
            return {}
            
        # æå–æ‰€æœ‰æœ‰æ•ˆçš„JSONå¯¹è±¡
        json_objects = self._extract_all_json_objects(text)
        
        if not json_objects:
            return {}
            
        if len(json_objects) == 1:
            return json_objects[0]
            
        # å¤šä¸ªJSONå¯¹è±¡çš„æƒ…å†µ - æ™ºèƒ½å¤„ç†
        first_obj = json_objects[0]
        
        # æ”¶é›†å…¶ä»–å¯¹è±¡çš„ä¿¡æ¯ï¼Œè®©Agentäº†è§£æƒ…å†µ
        other_info = []
        for obj in json_objects[1:]:
            if 'table_name' in obj:
                other_info.append(obj['table_name'])
            else:
                other_info.append('unnamed_table')
        
        # åœ¨ç¬¬ä¸€ä¸ªå¯¹è±¡ä¸­æ·»åŠ Agentåé¦ˆä¿¡æ¯
        first_obj['_agent_feedback'] = f"Multiple table requests detected. Currently processing '{first_obj.get('table_name', 'unknown')}'. Other tables found: {', '.join(other_info)}. Consider separate calls for each table."
        
        return first_obj
    
    def _extract_all_json_objects(self, text: str) -> list:
        """æå–æ‰€æœ‰æœ‰æ•ˆçš„JSONå¯¹è±¡"""
        objects = []
        brace_count = 0
        start_pos = -1
        
        for i, char in enumerate(text):
            if char == '{':
                if start_pos == -1:
                    start_pos = i
                brace_count += 1
            elif char == '}':
                brace_count -= 1
                if brace_count == 0 and start_pos != -1:
                    json_str = text[start_pos:i+1]
                    try:
                        obj = json.loads(json_str)
                        objects.append(obj)
                    except:
                        pass
                    start_pos = -1
        
        return objects
        
    def _create_error_chunk(self, error_message: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯å“åº”å—"""
        return {
            "type": "error",
            "error": error_message,
            "text": f"Error: {error_message}"
        }
