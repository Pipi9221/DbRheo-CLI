"""
å·²ç»è¿‡æ—¶, å·²ç»è¿ç§»è‡³ gemini_service_new.py
Gemini APIæœåŠ¡ - å¤„ç†ä¸Google Gemini APIçš„é€šä¿¡
å®Œå…¨å¯¹é½Gemini CLIçš„APIè°ƒç”¨æ–¹å¼
"""

from ..utils.content_helper import get_parts, get_role, get_text
import os
from typing import List, Dict, Any, Optional, AsyncIterator
import google.generativeai as genai
from ..types.core_types import Content, PartListUnion, AbortSignal
from ..config.base import DatabaseConfig
from ..utils.debug_logger import DebugLogger
from ..utils.retry_with_backoff import retry_with_backoff, RetryOptions
from ..utils.debug_logger import log_info, DebugLogger


class GeminiService:
    """
    Gemini APIæœåŠ¡
    - ä¸Google Gemini APIçš„é€šä¿¡
    - æµå¼å“åº”å¤„ç†
    - é”™è¯¯å¤„ç†å’Œé‡è¯•
    - æ¨¡å‹é…ç½®ç®¡ç†
    """
    
    def __init__(self, config: DatabaseConfig):
        self.config = config
        self._setup_api()
        # ç¼“å­˜çš„æ¨¡å‹å®ä¾‹ï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
        self._cached_model = None
        self._cached_model_config = None
        # Tokenå»é‡æœºåˆ¶
        self._stream_token_tracker = None
        
    def _setup_api(self):
        """è®¾ç½®Gemini API"""
        api_key = self.config.get("google_api_key") or os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY environment variable is required")
            
        genai.configure(api_key=api_key)
        
        # é…ç½®æ¨¡å‹
        model_name = self.config.get_model() or "gemini-2.5-flash"
        
        # æ˜ å°„ç®€çŸ­åç§°åˆ°å®Œæ•´æ¨¡å‹åï¼ˆåªä¿ç•™æ ¸å¿ƒæ¨¡å‹ï¼‰
        model_mappings = {
            "gemini": "gemini-2.5-flash",  # ç¨³å®šç‰ˆæœ¬çš„æ­£å¼åç§°
            "flash": "gemini-2.5-flash",
            "gemini-flash": "gemini-2.5-flash",
            "gemini-2.5": "gemini-2.5-flash",
            "gemini-2.5-flash": "gemini-2.5-flash",
        }
        
        # å¦‚æœæ˜¯ç®€çŸ­åç§°ï¼Œè½¬æ¢ä¸ºå®Œæ•´åç§°
        for short_name, full_name in model_mappings.items():
            if model_name.lower() == short_name.lower():
                self.model_name = full_name
                break
        else:
            # ä½¿ç”¨åŸå§‹åç§°
            self.model_name = model_name
        
        # é»˜è®¤ç”Ÿæˆé…ç½®
        self.default_generation_config = {
            "temperature": 0.7,
            "top_p": 0.8,
            "top_k": 40,
            "max_output_tokens": 8192,
        }
        
        # thinkingåŠŸèƒ½ç›®å‰ä¸è¢«Google AI SDKæ”¯æŒ
        # self.thinking_config = {
        #     "thinking_budget": 0  # è®¾ç½®ä¸º0å…³é—­thinking
        # }
        
    def send_message_stream(
        self,
        contents: List[Content],
        tools: Optional[List[Dict[str, Any]]] = None,
        system_instruction: Optional[str] = None,
        signal: Optional[AbortSignal] = None
    ):
        """
        å‘é€æ¶ˆæ¯å¹¶è¿”å›æµå¼å“åº”ï¼ˆåŒæ­¥ç”Ÿæˆå™¨ï¼‰
        å®Œå…¨å¯¹é½Gemini CLIçš„APIè°ƒç”¨æ–¹å¼
        """
        try:
            # è°ƒè¯•ï¼šæ‰“å°è°ƒç”¨ä¿¡æ¯
            log_info("Gemini", f"send_message_stream called")
            log_info("Gemini", f"History length: {len(contents)} messages")
            log_info("Gemini", f"System instruction length: {len(system_instruction) if system_instruction else 0} chars")
            log_info("Gemini", f"Tools count: {len(tools) if tools else 0}")
            
            # è®¡ç®—å†å²å†…å®¹çš„æ€»å­—ç¬¦æ•°
            total_chars = sum(
                sum(len(get_text(part)) for part in get_parts(msg))
                for msg in contents
            )
            log_info("Gemini", f"Total history content: {total_chars} chars")
            
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            request_contents = self._prepare_contents(contents)
            
            # è·å–æˆ–åˆ›å»ºç¼“å­˜çš„æ¨¡å‹å®ä¾‹
            model = self._get_or_create_model(system_instruction, tools)
            
            # ä½¿ç”¨é»˜è®¤çš„ç”Ÿæˆé…ç½®
            generation_config = self.default_generation_config.copy()
            
            # ä½¿ç”¨é‡è¯•æœºåˆ¶å‘é€æ¶ˆæ¯
            from ..utils.retry_with_backoff import retry_with_backoff_sync
            
            def api_call():
                return model.generate_content(
                    request_contents,
                    generation_config=generation_config,
                    stream=True
                )
            
            # é…ç½®é‡è¯•é€‰é¡¹
            retry_options = RetryOptions(
                max_attempts=3,  # å¯¹äºæµå¼å“åº”ï¼Œå‡å°‘é‡è¯•æ¬¡æ•°
                initial_delay_ms=2000,
                max_delay_ms=10000
            )
            
            response = retry_with_backoff_sync(api_call, retry_options)
            
            # å¤„ç†æµå¼å“åº”
            chunk_count = 0
            self._chunk_count = 0  # é‡ç½®chunkè®¡æ•°å™¨
            self._stream_token_tracker = None  # é‡ç½®tokenè·Ÿè¸ªå™¨
            final_chunk = None  # è·Ÿè¸ªæœ€åä¸€ä¸ªchunk
            
            for chunk in response:
                chunk_count += 1
                final_chunk = chunk  # ä¿å­˜æ¯ä¸ªchunkï¼Œæœ€åä¸€ä¸ªå°±æ˜¯æœ€ç»ˆchunk
                
                if signal and signal.aborted:
                    break
                    
                processed = self._process_chunk(chunk)
                DebugLogger.log_gemini_chunk(chunk_count, chunk, processed)
                yield processed
                
            # è°ƒè¯•ï¼šæµç»“æŸæ—¶çš„æ€»ç»“
            log_info("Gemini", f"ğŸ” TOKEN DEBUG - Stream ended. Total chunks: {chunk_count}")
            
            # åœ¨æµç»“æŸåï¼Œå‘é€æœ€ç»ˆçš„tokenç»Ÿè®¡
            if self._stream_token_tracker and final_chunk:
                log_info("Gemini", f"ğŸ¯ FINAL TOKEN USAGE - Sending final token statistics")
                yield {
                    "token_usage": self._stream_token_tracker,
                    "_final_token_report": True  # æ ‡è®°è¿™æ˜¯æœ€ç»ˆæŠ¥å‘Š
                }
                
        except Exception as e:
            # é”™è¯¯å¤„ç† - è®°å½•å®Œæ•´é”™è¯¯ä¿¡æ¯
            log_error("Gemini", f"API error: {type(e).__name__}: {str(e)}")
            
            # åœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºå®Œæ•´é”™è¯¯ï¼Œå¦åˆ™æ˜¾ç¤ºå‹å¥½æç¤º
            if DebugLogger.should_log("DEBUG"):
                error_message = f"Gemini API error: {type(e).__name__}: {str(e)}"
            else:
                error_message = "Gemini API is temporarily unstable. Please try again."
            
            yield self._create_error_chunk(error_message)
            
    async def generate_json(
        self,
        contents: List[Content],
        schema: Dict[str, Any],
        signal: Optional[AbortSignal] = None,
        system_instruction: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ç”ŸæˆJSONå“åº” - ç”¨äºnext_speakeråˆ¤æ–­ç­‰ç»“æ„åŒ–è¾“å‡º
        """
        try:
            # å‡†å¤‡è¯·æ±‚
            request_contents = self._prepare_contents(contents)
            
            # é…ç½®JSONæ¨¡å¼
            generation_config = {
                "temperature": 0.1,  # é™ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
                "response_mime_type": "application/json",
                "response_schema": schema
            }
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            model_config = {}
            if system_instruction:
                model_config['system_instruction'] = system_instruction
                
            model = genai.GenerativeModel(
                model_name=self.model_name,
                **model_config
            )
            
            # ä½¿ç”¨é‡è¯•æœºåˆ¶å‘é€è¯·æ±‚
            from ..utils.retry_with_backoff import retry_with_backoff_sync
            
            def api_call():
                return model.generate_content(
                    request_contents,
                    generation_config=generation_config
                )
            
            # é…ç½®é‡è¯•é€‰é¡¹
            retry_options = RetryOptions(
                max_attempts=5,
                initial_delay_ms=3000,
                max_delay_ms=20000
            )
            
            response = retry_with_backoff_sync(api_call, retry_options)
            
            # è§£æJSONå“åº”
            import json
            return json.loads(response.text)
            
        except Exception as e:
            # è¿”å›é»˜è®¤å“åº”
            return {
                "next_speaker": "user",
                "reasoning": f"Error in JSON generation: {str(e)}"
            }
            
    def _prepare_contents(self, contents: List[Content]) -> List[Dict[str, Any]]:
        """å‡†å¤‡APIè¯·æ±‚çš„å†…å®¹æ ¼å¼"""
        prepared = []
        for content in contents:
            # é˜²å¾¡æ€§æ£€æŸ¥ï¼šå¦‚æœæ˜¯ protobuf å¯¹è±¡ï¼Œå…ˆè½¬æ¢ä¸ºå­—å…¸
            if hasattr(content, '_pb'):
                content_dict = {
                    'role': content.role,
                    'parts': []
                }
                for part in content.parts:
                    if hasattr(part, 'text'):
                        content_dict['parts'].append({'text': part.text})
                    elif hasattr(part, 'function_call'):
                        # é€’å½’è½¬æ¢åµŒå¥—çš„ protobuf å¯¹è±¡
                        fc_dict = {}
                        if hasattr(part.function_call, '__dict__'):
                            for key, value in part.function_call.__dict__.items():
                                if not key.startswith('_'):
                                    fc_dict[key] = value
                        content_dict['parts'].append({'function_call': fc_dict})
                    elif hasattr(part, 'function_response'):
                        fr_dict = {}
                        if hasattr(part.function_response, '__dict__'):
                            for key, value in part.function_response.__dict__.items():
                                if not key.startswith('_'):
                                    fr_dict[key] = value
                        content_dict['parts'].append({'function_response': fr_dict})
                content = content_dict
            
            prepared_content = {
                "role": content["role"],
                "parts": []
            }
            
            for part in get_parts(content):
                if get_text(part):
                    prepared_content["parts"].append({"text": part["text"]})
                elif part.get("function_call"):
                    prepared_content["parts"].append({"function_call": part["function_call"]})
                elif part.get("function_response"):
                    prepared_content["parts"].append({"function_response": part["function_response"]})
                elif part.get("functionResponse"):
                    # è½¬æ¢é©¼å³°å¼åˆ°ä¸‹åˆ’çº¿æ ¼å¼ï¼ˆPython SDK ä½¿ç”¨ function_responseï¼‰
                    prepared_content["parts"].append({"function_response": part["functionResponse"]})
                elif part.get("functionCall"):
                    # è½¬æ¢é©¼å³°å¼åˆ°ä¸‹åˆ’çº¿æ ¼å¼
                    prepared_content["parts"].append({"function_call": part["functionCall"]})
            
            # åªæœ‰å½“partsä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ åˆ°preparedåˆ—è¡¨
            # Gemini API ä¸å…è®¸ç©ºçš„ parts æ•°ç»„
            if prepared_content["parts"]:
                prepared.append(prepared_content)
            
        return prepared
        
    def _process_chunk(self, chunk) -> Dict[str, Any]:
        """å¤„ç†æµå¼å“åº”å—"""
        result = {}
        
        # è°ƒè¯•ï¼šè®°å½•chunkåºå·ï¼ˆéœ€è¦åœ¨è°ƒç”¨å¤„ä¼ å…¥ï¼‰
        if not hasattr(self, '_chunk_count'):
            self._chunk_count = 0
        self._chunk_count += 1
        
        # å®‰å…¨åœ°å°è¯•è·å–æ–‡æœ¬å†…å®¹
        # æ³¨æ„ï¼šå½“å“åº”åŒ…å« function_call æ—¶ï¼Œè®¿é—® chunk.text ä¼šæŠ›å‡ºå¼‚å¸¸
        try:
            if hasattr(chunk, 'text'):
                result["text"] = chunk.text
        except ValueError:
            # å¿½ç•¥ "Could not convert part.function_call to text" é”™è¯¯
            pass
            
        # å¤„ç†å‡½æ•°è°ƒç”¨ - ä» candidates[0].content.parts ä¸­æå–
        if hasattr(chunk, 'candidates') and chunk.candidates:
            candidate = chunk.candidates[0]
            if hasattr(candidate, 'content') and candidate.content:
                content = candidate.content
                if hasattr(content, 'parts') and content.parts:
                    function_calls = []
                    text_parts = []
                    
                    for part in content.parts:
                        # å¤„ç†å‡½æ•°è°ƒç”¨
                        if hasattr(part, 'function_call') and part.function_call:
                            call = part.function_call
                            function_calls.append({
                                "id": getattr(call, 'id', f"call_{len(function_calls)}"),
                                "name": call.name,
                                "args": dict(call.args) if hasattr(call, 'args') else {}
                            })
                        # å¤„ç†æ–‡æœ¬ï¼ˆå¦‚æœæ²¡æœ‰ä» chunk.text è·å–åˆ°ï¼‰
                        elif hasattr(part, 'text') and part.text and not get_text(result):
                            text_parts.append(part.text)
                    
                    # åˆå¹¶æ–‡æœ¬éƒ¨åˆ†
                    if text_parts and not get_text(result):
                        result["text"] = "".join(text_parts)
                    
                    # åªåœ¨æœ‰å‡½æ•°è°ƒç”¨æ—¶æ·»åŠ function_callså­—æ®µ
                    if function_calls:
                        result["function_calls"] = function_calls
        
        # æ£€æŸ¥ token ä½¿ç”¨ä¿¡æ¯ - æœ€å°ä¾µå…¥æ€§æ·»åŠ 
        # Gemini API çš„ usage_metadata é€šå¸¸åœ¨æœ€åä¸€ä¸ª chunk ä¸­
        if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
            usage = chunk.usage_metadata
            token_info = {
                "prompt_tokens": getattr(usage, 'prompt_token_count', 0),
                "completion_tokens": getattr(usage, 'candidates_token_count', 0),
                "total_tokens": getattr(usage, 'total_token_count', 0),
                "cached_tokens": getattr(usage, 'cached_content_token_count', 0)  # æ–°å¢ï¼šGemini 2.5çš„ç¼“å­˜token
            }
            
            # æ›´æ–°è·Ÿè¸ªå™¨ï¼ˆæ€»æ˜¯ä¿å­˜æœ€æ–°çš„å€¼ï¼‰
            self._stream_token_tracker = token_info
            
            # è¯¦ç»†è°ƒè¯•ä¿¡æ¯
            log_info("Gemini", f"ğŸ” TOKEN DEBUG - Chunk #{self._chunk_count} has usage_metadata:")
            log_info("Gemini", f"   - prompt_tokens: {token_info['prompt_tokens']}")
            log_info("Gemini", f"   - completion_tokens: {token_info['completion_tokens']}")
            log_info("Gemini", f"   - total_tokens: {token_info['total_tokens']}")
            log_info("Gemini", f"   - cached_tokens: {token_info['cached_tokens']}")
            if token_info['cached_tokens'] > 0:
                log_info("Gemini", f"   - Cache hit: {token_info['cached_tokens']} tokens cached")
            # è°ƒè¯•ï¼šåˆ—å‡ºusage_metadataçš„æ‰€æœ‰å±æ€§å’Œå€¼
            if DebugLogger.should_log("DEBUG"):
                attrs = [attr for attr in dir(usage) if not attr.startswith('_')]
                log_info("Gemini", f"   - usage_metadata attributes: {attrs}")
                # å°è¯•ç›´æ¥è®¿é—®cached_content_token_count
                try:
                    cached_raw = usage.cached_content_token_count
                    log_info("Gemini", f"   - Raw cached_content_token_count: {cached_raw}")
                except:
                    log_info("Gemini", "   - cached_content_token_count not accessible")
            log_info("Gemini", f"   - From chunk.usage_metadata directly")
            log_info("Gemini", f"   - ğŸš« NOT sending token event (will send at stream end)")
            
            # ä¸å†åœ¨è¿™é‡Œè¿”å›token_usageï¼Œç­‰å¾…æµç»“æŸ
        # ä¹Ÿæ£€æŸ¥ candidates ä¸­çš„ usage_metadata
        elif hasattr(chunk, 'candidates') and chunk.candidates:
            for idx, candidate in enumerate(chunk.candidates):
                if hasattr(candidate, 'usage_metadata') and candidate.usage_metadata:
                    usage = candidate.usage_metadata
                    token_info = {
                        "prompt_tokens": getattr(usage, 'prompt_token_count', 0),
                        "completion_tokens": getattr(usage, 'candidates_token_count', 0),
                        "total_tokens": getattr(usage, 'total_token_count', 0),
                        "cached_tokens": getattr(usage, 'cached_content_token_count', 0)  # æ–°å¢
                    }
                    
                    # æ›´æ–°è·Ÿè¸ªå™¨
                    self._stream_token_tracker = token_info
                    
                    # è¯¦ç»†è°ƒè¯•ä¿¡æ¯
                    log_info("Gemini", f"ğŸ” TOKEN DEBUG - Chunk #{self._chunk_count} has usage_metadata in candidate[{idx}]:")
                    log_info("Gemini", f"   - prompt_tokens: {token_info['prompt_tokens']}")
                    log_info("Gemini", f"   - completion_tokens: {token_info['completion_tokens']}")
                    log_info("Gemini", f"   - total_tokens: {token_info['total_tokens']}")
                    log_info("Gemini", f"   - From chunk.candidates[{idx}].usage_metadata")
                    log_info("Gemini", f"   - ğŸš« NOT sending token event (will send at stream end)")
                    
                    # ä¸å†åœ¨è¿™é‡Œè¿”å›token_usage
                    break
            
        return result
        
    def _create_error_chunk(self, error_message: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯å“åº”å—"""
        return {
            "type": "error",
            "error": error_message,
            "text": f"Error: {error_message}"
        }
    
    def _get_or_create_model(
        self, 
        system_instruction: Optional[str] = None,
        tools: Optional[List[Dict[str, Any]]] = None
    ) -> genai.GenerativeModel:
        """
        è·å–æˆ–åˆ›å»ºç¼“å­˜çš„æ¨¡å‹å®ä¾‹
        åªæœ‰åœ¨é…ç½®æ”¹å˜æ—¶æ‰é‡æ–°åˆ›å»º
        """
        # æ„å»ºå½“å‰é…ç½®
        model_config = {}
        if system_instruction:
            model_config['system_instruction'] = system_instruction
            
        # å‡†å¤‡å·¥å…·é…ç½®
        enable_code_execution = self.config.get("enable_code_execution", False)
        
        if enable_code_execution and tools:
            # å¦‚æœåŒæ—¶å¯ç”¨äº†ä»£ç æ‰§è¡Œå’Œå‡½æ•°å·¥å…·ï¼Œä¼˜å…ˆä½¿ç”¨å‡½æ•°å·¥å…·
            print("[INFO Gemini] Code execution enabled but using function tools - code will be executed in conversation")
            model_config['tools'] = [{
                "function_declarations": tools
            }]
        elif enable_code_execution and not tools:
            # åªæœ‰ä»£ç æ‰§è¡Œï¼Œæ²¡æœ‰å‡½æ•°å·¥å…·
            model_config['tools'] = [{
                "code_execution": {}
            }]
        elif tools:
            # åªæœ‰å‡½æ•°å·¥å…·
            model_config['tools'] = [{
                "function_declarations": tools
            }]
        
        # æ£€æŸ¥é…ç½®æ˜¯å¦æ”¹å˜
        config_changed = (
            self._cached_model is None or 
            self._cached_model_config != model_config
        )
        
        # å¦‚æœé…ç½®æ”¹å˜æˆ–æ²¡æœ‰ç¼“å­˜çš„æ¨¡å‹ï¼Œåˆ›å»ºæ–°çš„
        if config_changed:
            self._cached_model = genai.GenerativeModel(
                model_name=self.model_name,
                **model_config
            )
            self._cached_model_config = model_config.copy()
            log_info("Gemini", "Created new GenerativeModel instance with cached config")
        
        return self._cached_model
    
