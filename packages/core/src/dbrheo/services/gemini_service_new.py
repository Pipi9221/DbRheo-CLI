"""
Gemini APIæœåŠ¡ - ä½¿ç”¨æ–°ç‰ˆgoogle-genai SDK
ä¿æŒä¸åŸæœ‰æ¥å£å®Œå…¨å…¼å®¹ï¼Œæœ€å°ä¾µå…¥æ€§è¿ç§»
"""

import os
import warnings
from typing import List, Dict, Any, Optional, AsyncIterator
try:
    from google import genai
    from google.genai import types
except ImportError as e:
    raise ImportError(
        "è¯·å®‰è£…æ–°ç‰ˆGemini SDK: pip install google-genai>=1.0.0"
    ) from e

from ..types.core_types import Content, PartListUnion, AbortSignal
from ..config.base import DatabaseConfig
from ..utils.debug_logger import DebugLogger
from ..utils.retry_with_backoff import retry_with_backoff, RetryOptions
from ..utils.debug_logger import log_info, DebugLogger


class GeminiService:
    """
    Gemini APIæœåŠ¡ - ä½¿ç”¨æ–°ç‰ˆgoogle-genai SDK
    - ä¸Google Gemini APIçš„é€šä¿¡
    - æµå¼å“åº”å¤„ç†
    - é”™è¯¯å¤„ç†å’Œé‡è¯•
    - æ¨¡å‹é…ç½®ç®¡ç†
    - ä¿æŒä¸æ—§ç‰ˆæ¥å£å®Œå…¨å…¼å®¹
    """
    
    def __init__(self, config: DatabaseConfig):
        self.config = config
        # ç¼“å­˜çš„é…ç½®ï¼ˆç”¨äºæ£€æµ‹æ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆï¼‰
        self._cached_model_config = None
        # Tokenå»é‡æœºåˆ¶
        self._stream_token_tracker = None
        # Clientå®ä¾‹
        self._client = None
        # æ˜¾å¼ç¼“å­˜ç›¸å…³
        self._explicit_cache = None  # ç¼“å­˜å¯¹è±¡
        self._cache_key = None  # ç¼“å­˜å†…å®¹çš„æ ‡è¯†
        # åˆå§‹åŒ–API
        self._setup_api()
        
    def _setup_api(self):
        """è®¾ç½®Gemini API - ä½¿ç”¨æ–°SDK"""
        # è·å–APIå¯†é’¥ - æ–°SDKæ”¯æŒä¸¤ä¸ªç¯å¢ƒå˜é‡
        api_key = (
            self.config.get("google_api_key") or 
            os.getenv("GOOGLE_API_KEY") or
            os.getenv("GEMINI_API_KEY")
        )
        if not api_key:
            # ä¸åœ¨åˆå§‹åŒ–æ—¶æŠ›å‡ºé”™è¯¯ï¼Œè€Œæ˜¯åœ¨å®é™…ä½¿ç”¨æ—¶æ‰æ£€æŸ¥
            self._api_key_missing = True
            self._client = None
            return
        
        self._api_key_missing = False
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        self._client = genai.Client(api_key=api_key)
        
        # é…ç½®æ¨¡å‹
        model_name = self.config.get_model() or "gemini-2.5-flash"
        
        # è°ƒè¯•
        log_info("Gemini", f"config.get_model()è¿”å›: {self.config.get_model()}")
        log_info("Gemini", f"ä½¿ç”¨çš„model_name: {model_name}")
        
        # æ˜ å°„ç®€çŸ­åç§°åˆ°å®Œæ•´æ¨¡å‹åï¼ˆåªä¿ç•™æ ¸å¿ƒæ¨¡å‹ï¼‰
        model_mappings = {
            "gemini": "gemini-2.5-flash",  # ç¨³å®šç‰ˆæœ¬çš„æ­£å¼åç§°
            "flash": "gemini-2.5-flash",
            "gemini-flash": "gemini-2.5-flash",
            "gemini-2.5": "gemini-2.5-flash",
            "gemini-2.5-flash": "gemini-2.5-flash",
        }
        
        # å¦‚æœæ˜¯ç®€çŸ­åç§°ï¼Œè½¬æ¢ä¸ºå®Œæ•´åç§°
        for short_name, full_name in model_mappings.items():
            if model_name.lower() == short_name.lower():
                self.model_name = full_name
                log_info("Gemini", f"æ˜ å°„ {model_name} -> {self.model_name}")
                break
        else:
            # ä½¿ç”¨åŸå§‹åç§°
            self.model_name = model_name
            log_info("Gemini", f"ä½¿ç”¨åŸå§‹åç§°: {self.model_name}")
        
        # é»˜è®¤ç”Ÿæˆé…ç½®
        self.default_generation_config = {
            "temperature": 0.7,
            "top_p": 0.8,
            "top_k": 40,
            "max_output_tokens": 8192,
        }
        
    def _get_cache_key(self, system_instruction: Optional[str], tools: Optional[List[Dict[str, Any]]]) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”® - åŸºäºç³»ç»ŸæŒ‡ä»¤å’Œå·¥å…·çš„å“ˆå¸Œ
        """
        import hashlib
        import json
        
        # ç»„åˆç³»ç»ŸæŒ‡ä»¤å’Œå·¥å…·ç”Ÿæˆå”¯ä¸€æ ‡è¯†
        cache_content = {
            "system_instruction": system_instruction or "",
            "tools": tools or []
        }
        
        # ç”Ÿæˆç¨³å®šçš„å“ˆå¸Œå€¼
        content_str = json.dumps(cache_content, sort_keys=True)
        return hashlib.sha256(content_str.encode()).hexdigest()[:16]
        
    def _ensure_explicit_cache(self, system_instruction: Optional[str], tools: Optional[List[Dict[str, Any]]]) -> Optional[str]:
        """
        ç¡®ä¿æ˜¾å¼ç¼“å­˜å­˜åœ¨å¹¶è¿”å›ç¼“å­˜åç§°
        å¦‚æœç¼“å­˜åˆ›å»ºå¤±è´¥ï¼Œè¿”å› Noneï¼ˆå›é€€åˆ°æ™®é€šè¯·æ±‚ï¼‰
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ˜¾å¼ç¼“å­˜
        enable_cache = self.config.get("enable_explicit_cache", True)
        log_info("Gemini", f"Explicit cache enabled: {enable_cache}")
        
        if not enable_cache:
            log_info("Gemini", "Explicit cache is disabled")
            return None
            
        # è®¡ç®—å½“å‰å†…å®¹çš„ç¼“å­˜é”®
        current_key = self._get_cache_key(system_instruction, tools)
        log_info("Gemini", f"Cache key: {current_key}")
        log_info("Gemini", f"Previous cache key: {self._cache_key}")
        log_info("Gemini", f"Existing cache: {self._explicit_cache}")
        
        # å¦‚æœç¼“å­˜é”®æ²¡å˜ä¸”ç¼“å­˜å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        if current_key == self._cache_key and self._explicit_cache:
            log_info("Gemini", f"Using existing explicit cache: {self._explicit_cache}")
            return self._explicit_cache
            
        # éœ€è¦åˆ›å»ºæ–°ç¼“å­˜
        try:
            log_info("Gemini", "Creating new explicit cache...")
            
            # å‡†å¤‡ç¼“å­˜é…ç½®
            cache_config_dict = {
                'display_name': f'dbrheo_cache_{current_key}',
                'system_instruction': system_instruction,
                'ttl': "3600s"  # 1å°æ—¶ TTL
            }
            
            # å¦‚æœæœ‰å·¥å…·ï¼Œéœ€è¦è½¬æ¢æ ¼å¼
            if tools:
                # å°è¯•ç›´æ¥ä½¿ç”¨å·¥å…·å£°æ˜
                # å¦‚æœå¤±è´¥ï¼Œå¯èƒ½éœ€è¦è½¬æ¢ä¸º Tool å¯¹è±¡
                try:
                    # é¦–å…ˆå°è¯•åˆ›å»º Tool å¯¹è±¡
                    # æ‰€æœ‰å‡½æ•°å£°æ˜åº”è¯¥åœ¨ä¸€ä¸ª Tool å¯¹è±¡ä¸­
                    function_declarations = []
                    for tool_dict in tools:
                        # åˆ›å»ºå‡½æ•°å£°æ˜
                        function_declaration = types.FunctionDeclaration(
                            name=tool_dict['name'],
                            description=tool_dict['description'],
                            parameters=tool_dict['parameters']
                        )
                        function_declarations.append(function_declaration)
                    
                    # åˆ›å»ºå•ä¸ª Tool å¯¹è±¡åŒ…å«æ‰€æœ‰å‡½æ•°
                    tool_object = types.Tool(function_declarations=function_declarations)
                    cache_config_dict['tools'] = [tool_object]
                except:
                    # å¦‚æœå¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨åŸå§‹æ ¼å¼
                    cache_config_dict['tools'] = tools
                
            cache_config = types.CreateCachedContentConfig(**cache_config_dict)
            
            # åˆ›å»ºç¼“å­˜
            cache = self._client.caches.create(
                model=self.model_name,
                config=cache_config
            )
            
            # æ›´æ–°ç¼“å­˜ä¿¡æ¯
            self._explicit_cache = cache.name
            self._cache_key = current_key
            
            log_info("Gemini", f"âœ“ Explicit cache created: {cache.name}")
            return cache.name
            
        except Exception as e:
            # ç¼“å­˜åˆ›å»ºå¤±è´¥ï¼Œè®°å½•ä½†ä¸å½±å“æ­£å¸¸åŠŸèƒ½
            log_error("Gemini", f"Failed to create explicit cache: {e}")
            return None
        
    def send_message_stream(
        self,
        contents: List[Content],
        tools: Optional[List[Dict[str, Any]]] = None,
        system_instruction: Optional[str] = None,
        signal: Optional[AbortSignal] = None
    ):
        """
        å‘é€æ¶ˆæ¯å¹¶è¿”å›æµå¼å“åº”ï¼ˆåŒæ­¥ç”Ÿæˆå™¨ï¼‰
        å®Œå…¨ä¿æŒåŸæœ‰æ¥å£ä¸å˜
        """
        # åœ¨å®é™…ä½¿ç”¨æ—¶æ£€æŸ¥API key
        if getattr(self, '_api_key_missing', False):
            raise ValueError("GOOGLE_API_KEY or GEMINI_API_KEY environment variable is required")
        
        try:
            # è°ƒè¯•ï¼šæ‰“å°è°ƒç”¨ä¿¡æ¯
            log_info("Gemini", f"send_message_stream called (new SDK)")
            log_info("Gemini", f"History length: {len(contents)} messages")
            log_info("Gemini", f"System instruction length: {len(system_instruction) if system_instruction else 0} chars")
            log_info("Gemini", f"Tools count: {len(tools) if tools else 0}")
            
            # è®¡ç®—å†å²å†…å®¹çš„æ€»å­—ç¬¦æ•°
            from ..utils.content_helper import get_parts, get_text
            total_chars = sum(
                sum(len(get_text(part)) for part in get_parts(msg))
                for msg in contents
            )
            log_info("Gemini", f"Total history content: {total_chars} chars")
            
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            request_contents = self._prepare_contents(contents)
            
            # å°è¯•ä½¿ç”¨æ˜¾å¼ç¼“å­˜
            cached_content_name = self._ensure_explicit_cache(system_instruction, tools)
            
            # æ„å»ºé…ç½®
            config = self._build_generate_config(
                system_instruction=system_instruction,
                tools=tools,
                generation_config=self.default_generation_config.copy(),
                cached_content=cached_content_name  # ä¼ é€’ç¼“å­˜åç§°
            )
            
            # ä½¿ç”¨é‡è¯•æœºåˆ¶å‘é€æ¶ˆæ¯
            from ..utils.retry_with_backoff import retry_with_backoff_sync
            
            def api_call():
                # æ–°SDKçš„æµå¼API
                return self._client.models.generate_content_stream(
                    model=self.model_name,
                    contents=request_contents,
                    config=config
                )
            
            # é…ç½®é‡è¯•é€‰é¡¹
            retry_options = RetryOptions(
                max_attempts=3,  # å¯¹äºæµå¼å“åº”ï¼Œå‡å°‘é‡è¯•æ¬¡æ•°
                initial_delay_ms=2000,
                max_delay_ms=10000
            )
            
            response_stream = retry_with_backoff_sync(api_call, retry_options)
            
            # å¤„ç†æµå¼å“åº”
            chunk_count = 0
            self._chunk_count = 0  # é‡ç½®chunkè®¡æ•°å™¨
            self._stream_token_tracker = None  # é‡ç½®tokenè·Ÿè¸ªå™¨
            final_chunk = None  # è·Ÿè¸ªæœ€åä¸€ä¸ªchunk
            
            for chunk in response_stream:
                chunk_count += 1
                final_chunk = chunk  # ä¿å­˜æ¯ä¸ªchunkï¼Œæœ€åä¸€ä¸ªå°±æ˜¯æœ€ç»ˆchunk
                
                if signal and signal.aborted:
                    break
                    
                processed = self._process_chunk(chunk)
                DebugLogger.log_gemini_chunk(chunk_count, chunk, processed)
                yield processed
                
            # è°ƒè¯•ï¼šæµç»“æŸæ—¶çš„æ€»ç»“
            log_info("Gemini", f"ğŸ” TOKEN DEBUG - Stream ended. Total chunks: {chunk_count}")
            
            # åœ¨æµç»“æŸåï¼Œå‘é€æœ€ç»ˆçš„tokenç»Ÿè®¡
            if self._stream_token_tracker:
                log_info("Gemini", f"ğŸ¯ FINAL TOKEN USAGE - Sending final token statistics")
                log_info("Gemini", f"   - Final stats: {self._stream_token_tracker}")
                yield {
                    "token_usage": self._stream_token_tracker,
                    "_final_token_report": True  # æ ‡è®°è¿™æ˜¯æœ€ç»ˆæŠ¥å‘Š
                }
                
        except Exception as e:
            # é”™è¯¯å¤„ç† - è®°å½•å®Œæ•´é”™è¯¯ä¿¡æ¯
            log_error("Gemini", f"API error: {type(e).__name__}: {str(e)}")
            
            # åœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºå®Œæ•´é”™è¯¯ï¼Œå¦åˆ™æ˜¾ç¤ºå‹å¥½æç¤º
            if DebugLogger.should_log("DEBUG"):
                error_message = f"Gemini API error: {type(e).__name__}: {str(e)}"
            else:
                error_message = "Gemini API is temporarily unstable. Please try again."
            
            yield self._create_error_chunk(error_message)
            
    async def generate_json(
        self,
        contents: List[Content],
        schema: Dict[str, Any],
        signal: Optional[AbortSignal] = None,
        system_instruction: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ç”ŸæˆJSONå“åº” - ç”¨äºnext_speakeråˆ¤æ–­ç­‰ç»“æ„åŒ–è¾“å‡º
        ä¿æŒåŸæœ‰æ¥å£ä¸å˜
        """
        try:
            # å‡†å¤‡è¯·æ±‚
            request_contents = self._prepare_contents(contents)
            
            # é…ç½®JSONæ¨¡å¼
            generation_config = types.GenerateContentConfig(
                temperature=0.1,  # é™ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
                response_mime_type="application/json",
                response_schema=schema,
                system_instruction=system_instruction
            )
            
            # ä½¿ç”¨é‡è¯•æœºåˆ¶å‘é€è¯·æ±‚
            from ..utils.retry_with_backoff import retry_with_backoff_sync
            
            def api_call():
                # æ–°SDKçš„åŒæ­¥API
                return self._client.models.generate_content(
                    model=self.model_name,
                    contents=request_contents,
                    config=generation_config
                )
            
            # é…ç½®é‡è¯•é€‰é¡¹
            retry_options = RetryOptions(
                max_attempts=5,
                initial_delay_ms=3000,
                max_delay_ms=20000
            )
            
            response = retry_with_backoff_sync(api_call, retry_options)
            
            # è§£æJSONå“åº”
            import json
            return json.loads(response.text)
            
        except Exception as e:
            # è¿”å›é»˜è®¤å“åº”
            return {
                "next_speaker": "user",
                "reasoning": f"Error in JSON generation: {str(e)}"
            }
            
    def _prepare_contents(self, contents: List[Content]) -> List[Dict[str, Any]]:
        """å‡†å¤‡APIè¯·æ±‚çš„å†…å®¹æ ¼å¼ - ä¸åŸç‰ˆä¿æŒä¸€è‡´"""
        prepared = []
        for content in contents:
            # è½¬æ¢ä¸ºå­—å…¸ï¼ˆæ”¯æŒdictå’Œå¯¹è±¡ä¸¤ç§æ ¼å¼ï¼‰
            if isinstance(content, dict):
                content_dict = content
            else:
                # å¯¹è±¡æ ¼å¼ï¼Œè½¬æ¢ä¸ºå­—å…¸
                from ..utils.content_helper import get_parts, get_role
                content_dict = {
                    'role': get_role(content),
                    'parts': get_parts(content)
                }
            
            prepared_content = {
                "role": content_dict["role"],
                "parts": []
            }
            
            parts = content_dict.get("parts", [])
            for part in parts:
                # æ”¯æŒdictå’Œå¯¹è±¡ä¸¤ç§æ ¼å¼
                if isinstance(part, dict):
                    if part.get("text"):
                        prepared_content["parts"].append({"text": part["text"]})
                    elif part.get("function_call"):
                        prepared_content["parts"].append({"function_call": part["function_call"]})
                    elif part.get("function_response"):
                        prepared_content["parts"].append({"function_response": part["function_response"]})
                else:
                    # å¯¹è±¡æ ¼å¼
                    if hasattr(part, 'text') and part.text:
                        prepared_content["parts"].append({"text": part.text})
                    elif hasattr(part, 'function_call') and part.function_call:
                        prepared_content["parts"].append({"function_call": part.function_call})
                    elif hasattr(part, 'function_response') and part.function_response:
                        prepared_content["parts"].append({"function_response": part.function_response})
            
            # åªæœ‰å½“partsä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ åˆ°preparedåˆ—è¡¨
            if prepared_content["parts"]:
                prepared.append(prepared_content)
            
        return prepared
        
    def _process_chunk(self, chunk) -> Dict[str, Any]:
        """å¤„ç†æµå¼å“åº”å— - é€‚é…æ–°SDKçš„å“åº”æ ¼å¼"""
        result = {}
        
        # è°ƒè¯•ï¼šè®°å½•chunkåºå·
        if not hasattr(self, '_chunk_count'):
            self._chunk_count = 0
        self._chunk_count += 1
        
        # æ–°SDKä¸­ï¼Œæ–‡æœ¬ç›´æ¥åœ¨chunk.textå±æ€§
        if hasattr(chunk, 'text') and chunk.text:
            result["text"] = chunk.text
            
        # å¤„ç†å‡½æ•°è°ƒç”¨ - æ–°SDKå¯èƒ½æœ‰ä¸åŒçš„ç»“æ„
        if hasattr(chunk, 'candidates') and chunk.candidates:
            candidate = chunk.candidates[0]
            if hasattr(candidate, 'content') and candidate.content:
                content = candidate.content
                if hasattr(content, 'parts') and content.parts:
                    function_calls = []
                    
                    for part in content.parts:
                        # å¤„ç†å‡½æ•°è°ƒç”¨
                        if hasattr(part, 'function_call') and part.function_call:
                            call = part.function_call
                            
                            # æ›´ä»”ç»†åœ°æå–å‚æ•°
                            args = {}
                            if hasattr(call, 'args') and call.args is not None:
                                log_info("Gemini", f"Function call args type: {type(call.args)}")
                                log_info("Gemini", f"Function call args value: {call.args}")
                                
                                # æ–°SDKä¸­ args å·²ç»æ˜¯ dictï¼Œç›´æ¥ä½¿ç”¨
                                if isinstance(call.args, dict):
                                    args = call.args
                                else:
                                    # å…¼å®¹æ€§å¤„ç†
                                    try:
                                        args = dict(call.args)
                                    except Exception as e:
                                        log_error("Gemini", f"Failed to convert args to dict: {e}")
                                        log_error("Gemini", f"Args type: {type(call.args)}, value: {call.args}")
                            else:
                                log_info("Gemini", f"Function call has no args or args is None")
                            
                            # è°ƒè¯•ï¼šæ‰“å°æå–çš„å‚æ•°
                            log_info("Gemini", f"Extracted function call: {call.name}, args: {args}")
                            
                            function_calls.append({
                                "id": getattr(call, 'id', f"call_{len(function_calls)}"),
                                "name": call.name,
                                "args": args
                            })
                    
                    # åªåœ¨æœ‰å‡½æ•°è°ƒç”¨æ—¶æ·»åŠ function_callså­—æ®µ
                    if function_calls:
                        result["function_calls"] = function_calls
        
        # æ£€æŸ¥ token ä½¿ç”¨ä¿¡æ¯ - é€‚é…æ–°SDK
        usage_metadata = None
        
        # å°è¯•ä»chunkç›´æ¥è·å–
        if hasattr(chunk, 'usage_metadata'):
            usage_metadata = chunk.usage_metadata
        # å°è¯•ä»candidatesè·å–
        elif hasattr(chunk, 'candidates') and chunk.candidates:
            for candidate in chunk.candidates:
                if hasattr(candidate, 'usage_metadata') and candidate.usage_metadata:
                    usage_metadata = candidate.usage_metadata
                    break
                    
        if usage_metadata:
            # æ–°SDKä¸­ï¼Œcached_content_token_countå¯èƒ½æ˜¯Noneè€Œä¸æ˜¯0
            cached_count = getattr(usage_metadata, 'cached_content_token_count', None)
            
            # è°ƒè¯•ï¼šç›´æ¥è®¿é—®å±æ€§
            log_info("Gemini", f"ğŸ“Š CACHE DEBUG - Direct access:")
            log_info("Gemini", f"   - usage_metadata type: {type(usage_metadata)}")
            log_info("Gemini", f"   - cached_content_token_count: {usage_metadata.cached_content_token_count}")
            log_info("Gemini", f"   - getattr result: {cached_count}")
            
            # æ­£ç¡®è·å–ç¼“å­˜å€¼
            # æ³¨æ„ï¼šæ–°SDKä¸­cached_content_token_countå¯èƒ½æ˜¯å®é™…å€¼ï¼Œä¸ä¸€å®šæ˜¯None
            if cached_count is None:
                cached_count = 0
            else:
                # ç¡®ä¿æ˜¯æ•´æ•°
                cached_count = int(cached_count)
                
            token_info = {
                "prompt_tokens": getattr(usage_metadata, 'prompt_token_count', 0),
                "completion_tokens": getattr(usage_metadata, 'candidates_token_count', 0),
                "total_tokens": getattr(usage_metadata, 'total_token_count', 0),
                "cached_tokens": cached_count
            }
            
            # æ›´æ–°è·Ÿè¸ªå™¨ï¼ˆæ€»æ˜¯ä¿å­˜æœ€æ–°çš„å€¼ï¼‰
            self._stream_token_tracker = token_info
            
            # è¯¦ç»†è°ƒè¯•ä¿¡æ¯
            log_info("Gemini", f"ğŸ” TOKEN DEBUG - Chunk #{self._chunk_count} has usage_metadata:")
            log_info("Gemini", f"   - prompt_tokens: {token_info['prompt_tokens']}")
            log_info("Gemini", f"   - completion_tokens: {token_info['completion_tokens']}")
            log_info("Gemini", f"   - total_tokens: {token_info['total_tokens']}")
            log_info("Gemini", f"   - cached_tokens: {token_info['cached_tokens']} (raw: {cached_count})")
            if token_info['cached_tokens'] > 0:
                log_info("Gemini", f"   - âœ… Cache hit: {token_info['cached_tokens']} tokens cached")
            elif cached_count is None:
                log_info("Gemini", f"   - âš ï¸  cached_content_token_count is None (æ–°SDKçš„å·²çŸ¥é—®é¢˜)")
            log_info("Gemini", f"   - ğŸš« NOT sending token event (will send at stream end)")
            
        return result
        
    def _create_error_chunk(self, error_message: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯å“åº”å—"""
        return {
            "type": "error",
            "error": error_message,
            "text": f"Error: {error_message}"
        }
    
    def _build_generate_config(
        self, 
        system_instruction: Optional[str] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        generation_config: Optional[Dict[str, Any]] = None,
        cached_content: Optional[str] = None
    ) -> types.GenerateContentConfig:
        """
        æ„å»ºç”Ÿæˆé…ç½® - æ–°SDKä½¿ç”¨configå‚æ•°
        """
        config_dict = {}
        
        # å¦‚æœæœ‰ç¼“å­˜ï¼Œä½¿ç”¨ç¼“å­˜è€Œä¸æ˜¯ç³»ç»ŸæŒ‡ä»¤
        if cached_content:
            config_dict['cached_content'] = cached_content
            # ä½¿ç”¨ç¼“å­˜æ—¶ä¸éœ€è¦å†ä¼ system_instruction
        elif system_instruction:
            # æ²¡æœ‰ç¼“å­˜æ—¶æ‰ä½¿ç”¨ç³»ç»ŸæŒ‡ä»¤
            config_dict['system_instruction'] = system_instruction
            
        # ç”Ÿæˆå‚æ•°
        if generation_config:
            for key, value in generation_config.items():
                config_dict[key] = value
                
        # å·¥å…·é…ç½® - åªæœ‰åœ¨ä¸ä½¿ç”¨ç¼“å­˜æ—¶æ‰æ·»åŠ å·¥å…·
        if not cached_content:
            enable_code_execution = self.config.get("enable_code_execution", False)
            
            if enable_code_execution and tools:
                # å¦‚æœåŒæ—¶å¯ç”¨äº†ä»£ç æ‰§è¡Œå’Œå‡½æ•°å·¥å…·ï¼Œä¼˜å…ˆä½¿ç”¨å‡½æ•°å·¥å…·
                log_info("Gemini", "Code execution enabled but using function tools")
                # è½¬æ¢å·¥å…·æ ¼å¼
                try:
                    function_declarations = []
                    for tool_dict in tools:
                        function_declaration = types.FunctionDeclaration(
                            name=tool_dict['name'],
                            description=tool_dict['description'],
                            parameters=tool_dict['parameters']
                        )
                        function_declarations.append(function_declaration)
                    tool_object = types.Tool(function_declarations=function_declarations)
                    config_dict['tools'] = [tool_object]
                except:
                    config_dict['tools'] = tools
            elif enable_code_execution and not tools:
                # åªæœ‰ä»£ç æ‰§è¡Œï¼Œæ²¡æœ‰å‡½æ•°å·¥å…·
                # æ–°SDKä¸­ä»£ç æ‰§è¡Œçš„é…ç½®æ–¹å¼å¯èƒ½ä¸åŒï¼Œéœ€è¦æŸ¥çœ‹æ–‡æ¡£
                log_info("Gemini", "Code execution enabled (new SDK)")
                # config_dict['tools'] = [{"code_execution": {}}]  # å¾…ç¡®è®¤æ ¼å¼
            elif tools:
                # åªæœ‰å‡½æ•°å·¥å…·
                # è½¬æ¢å·¥å…·æ ¼å¼
                try:
                    function_declarations = []
                    for tool_dict in tools:
                        function_declaration = types.FunctionDeclaration(
                            name=tool_dict['name'],
                            description=tool_dict['description'],
                            parameters=tool_dict['parameters']
                        )
                        function_declarations.append(function_declaration)
                    tool_object = types.Tool(function_declarations=function_declarations)
                    config_dict['tools'] = [tool_object]
                except:
                    config_dict['tools'] = tools
        
        # è¿”å›é…ç½®å¯¹è±¡
        return types.GenerateContentConfig(**config_dict)
