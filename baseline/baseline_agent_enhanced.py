"""
Baseline Agent Enhanced - LLMç”Ÿæˆè¿‡æ»¤æ¡ä»¶
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../packages/core/src'))

import pandas as pd
from openai import OpenAI
import json
import re
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
env_file = os.path.abspath(os.path.join(os.path.dirname(__file__), '../.env'))
print(f"[OK] Load env: {env_file}")
load_dotenv(env_file)

class EnhancedBaselineAgent:
    """ä¼˜åŒ–çš„Baseline Agent - LLMç”Ÿæˆpandasè¿‡æ»¤æ¡ä»¶"""
    
    def __init__(self, csv_path: str):
        self.csv_path = csv_path
        self.df = pd.read_csv(csv_path)

        self.client = OpenAI(
            api_key=os.getenv('BASELINE_OPENAI_API_KEY'),
            base_url=os.getenv('BASELINE_OPENAI_API_BASE')
        )
        self.model = os.getenv('BASELINE_MODEL', 'qwen-flash')

        # è·å–æ•°æ®æ ·ä¾‹
        self.sample_data = self.df.head(5).to_string()
        self.columns = list(self.df.columns)

        print(f"[OK] Loaded CSV: {len(self.df)} rows")
        print(f"[OK] Columns: {self.columns}")
        print(f"[OK] Model: {self.model}")
    
    def _generate_filter_conditions(self, question: str, no_data_history: list = None) -> dict:
        """LLMç”Ÿæˆè¿‡æ»¤æ¡ä»¶ï¼ˆæå–æ—¶é—´ä¿¡æ¯å’Œå“ç‰Œ/è½¦å‹å…³é”®è¯ï¼‰"""
        feedback_msg = ""
        if no_data_history:
            feedback_msg = f"\n\nã€é‡è¦åé¦ˆã€‘\nä»¥ä¸‹æ—¶é—´èŒƒå›´æœªæ‰¾åˆ°æ•°æ®ï¼š{', '.join(no_data_history)}\nè¯·å°è¯•å…¶ä»–å¯èƒ½çš„æ—¶é—´èŒƒå›´æˆ–æ›´çµæ´»çš„æ—¶é—´è§£æç­–ç•¥ã€‚"

        prompt = f"""ä½ æ˜¯æ•°æ®è¿‡æ»¤ä¸“å®¶ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªCSVæ–‡ä»¶ï¼ŒåŒ…å«æ±½è½¦é”€é‡æ•°æ®ã€‚

ã€æ•°æ®ç»“æ„ã€‘
åˆ—å: {self.columns}

ã€æ•°æ®æ ·ä¾‹ã€‘
{self.sample_data}

ã€å­—æ®µè¯´æ˜ã€‘
- indicator_key: æŒ‡æ ‡ID
- display_name: æ ¼å¼ä¸º"æŒ‡æ ‡ç±»å‹ï¼šå“ç‰Œ_è½¦å‹ï¼šé¢‘ç‡"æˆ–"æŒ‡æ ‡ç±»å‹ï¼šå“ç‰Œ_è½¦å‹Aã€è½¦å‹Bï¼šé¢‘ç‡"
  * ç¤ºä¾‹ï¼š"ä¹˜ç”¨è½¦é”€é‡å¸‚åœºä»½é¢ï¼šå›½å†…åˆ¶é€ +CKD_1.0Lä»¥ä¸‹ï¼šæœˆ"ï¼ˆå¸‚åœºä»½é¢ç±»ï¼Œä½¿ç”¨+ï¼‰
  * ç¤ºä¾‹ï¼š"ä¹˜ç”¨è½¦é”€é‡ï¼šä¸Šæ±½é€šç”¨_å‡¯è¿ªæ‹‰å…‹CT5ã€CT6ï¼šæœˆ"ï¼ˆé”€é‡ç±»ï¼Œä½¿ç”¨_æˆ–ã€ï¼‰
  * ç¤ºä¾‹ï¼š"ä¹˜ç”¨è½¦é”€é‡ï¼šæ¯”äºšè¿ª_æµ·è±šï¼šæœˆ"ï¼ˆé”€é‡ç±»ï¼Œä½¿ç”¨_ï¼‰
- 89%çš„é”€é‡æ•°æ®ä½¿ç”¨"_"æˆ–"ã€"åˆ†éš”å“ç‰Œå’Œè½¦å‹
- å“ç‰Œå’Œè½¦å‹åœ¨"ï¼š"ä¹‹åã€"ï¼š"ä¹‹å‰ï¼Œç”¨"_"æˆ–"ã€"åˆ†éš”
- data_time: æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
{feedback_msg}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€ä»»åŠ¡ã€‘
åˆ†æé—®é¢˜ï¼Œæå–æ—¶é—´ä¿¡æ¯å’Œå“ç‰Œ/è½¦å‹å…³é”®è¯ã€‚è¿”å›JSONï¼š
{{
    "time_start": "å¼€å§‹æ—¶é—´ï¼ˆYYYY-MMæ ¼å¼ï¼Œå¦‚æ— åˆ™nullï¼‰",
    "time_end": "ç»“æŸæ—¶é—´ï¼ˆYYYY-MMæ ¼å¼ï¼Œå¦‚æ— åˆ™nullï¼‰",
    "need_comparison": true/false,
    "comparison_time": "å¯¹æ¯”æœŸæ—¶é—´ï¼ˆYYYY-MMæ ¼å¼ï¼Œå¦‚æ— åˆ™nullï¼‰",
    "brand_keywords": ["å“ç‰Œå…³é”®è¯1", "å“ç‰Œå…³é”®è¯2", ...],
    "model_keywords": ["è½¦å‹å…³é”®è¯1", "è½¦å‹å…³é”®è¯2", ...]
}}

ã€æå–è§„åˆ™ã€‘
1. æ—¶é—´æå–ï¼š
   - å¦‚æœé—®é¢˜åªæåˆ°ä¸€ä¸ªæ—¶é—´ç‚¹ï¼ˆå¦‚"2023-06"ï¼‰ï¼Œtime_startå’Œtime_endéƒ½è®¾ä¸ºè¯¥æ—¶é—´
   - å¦‚æœé—®é¢˜æåˆ°æ—¶é—´èŒƒå›´ï¼ˆå¦‚"2023å¹´ä¸ŠåŠå¹´"ï¼‰ï¼Œæå–å¼€å§‹å’Œç»“æŸæ—¶é—´
   - å¦‚æœé—®é¢˜æœªæ˜ç¡®æ—¶é—´ï¼Œtime_startå’Œtime_endéƒ½è®¾ä¸ºnull
   - å¦‚æœä¹‹å‰å°è¯•çš„æ—¶é—´æ²¡æœ‰æ•°æ®ï¼Œå°è¯•ç›¸é‚»æœˆä»½æˆ–æ›´å®½æ³›èŒƒå›´

2. å“ç‰Œ/è½¦å‹æå–ï¼š
   - ä»é—®é¢˜ä¸­æå–å“ç‰Œåï¼ˆå¦‚"ä¸€æ±½å¤§ä¼—"ã€"æ¯”äºšè¿ª"ã€"ä¸Šæ±½é€šç”¨"ç­‰ï¼‰
   - ä»é—®é¢˜ä¸­æå–è½¦å‹åï¼ˆå¦‚"æµ·è±š"ã€"æ½å¢ƒ"ã€"å‡¯è¿ªæ‹‰å…‹CT5"ç­‰ï¼‰
   - å¦‚æœé—®å“ç‰Œæ€»é”€é‡ï¼Œmodel_keywordsä¸ºç©ºæ•°ç»„
   - å¦‚æœé—®"å…¨ç³»"ã€"æ‰€æœ‰è½¦å‹"ç­‰ï¼Œmodel_keywordsä¸ºç©ºæ•°ç»„

3. åŒæ¯”/ç¯æ¯”ï¼š
   - åŒæ¯”ï¼šcomparison_timeè®¾ä¸ºå»å¹´åŒæœŸï¼ˆå¦‚2023-06çš„åŒæ¯”ï¼Œcomparison_time="2022-06"ï¼‰
   - ç¯æ¯”ï¼šcomparison_timeè®¾ä¸ºä¸ŠæœŸï¼ˆå¦‚2023-06çš„ç¯æ¯”ï¼Œcomparison_time="2023-05"ï¼‰
   - need_comparisonè®¾ä¸ºtrue

ã€é‡è¦ã€‘
- å¦‚æœæ‰¾ä¸åˆ°å“ç‰Œ/è½¦å‹ï¼Œbrand_keywordså’Œmodel_keywordséƒ½è®¾ä¸ºç©ºæ•°ç»„
- ç©ºæ•°ç»„è¡¨ç¤ºä¸è¿‡æ»¤å“ç‰Œ/è½¦å‹ï¼Œæ—¶é—´è¿‡æ»¤åæ‰€æœ‰æ•°æ®éƒ½ä¼ ç»™LLMåˆ†æ

åªè¿”å›JSONã€‚"""
        
        print(f"ğŸ§  LLMç”Ÿæˆè¿‡æ»¤æ¡ä»¶...")
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        
        result_text = response.choices[0].message.content.strip()
        json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
        if json_match:
            conditions = json.loads(json_match.group())
        else:
            conditions = json.loads(result_text)

        print(f"ğŸ“‹ è¿‡æ»¤æ¡ä»¶:")
        print(f"   å“ç‰Œå…³é”®è¯: {conditions.get('brand_keywords')}")
        print(f"   è½¦å‹å…³é”®è¯: {conditions.get('model_keywords')}")
        print(f"   æ—¶é—´èŒƒå›´: {conditions.get('time_start')} ~ {conditions.get('time_end')}")
        print(f"   éœ€è¦å¯¹æ¯”: {conditions.get('need_comparison')}")

        return conditions
    
    def _filter_data(self, conditions: dict) -> pd.DataFrame:
        """
        æ ¹æ®æ¡ä»¶è¿‡æ»¤æ•°æ®
        æ”¹è¿›ï¼šå…ˆæŒ‰æ—¶é—´è¿‡æ»¤ï¼Œå†æŒ‰å“ç‰Œ/è½¦å‹å…³é”®è¯è¿‡æ»¤ï¼Œå‡å°‘ä¼ ç»™LLMçš„æ•°æ®é‡
        """
        filtered = self.df.copy()

        # 1. æ—¶é—´è¿‡æ»¤ï¼ˆèŒƒå›´æŸ¥è¯¢ï¼‰
        time_start = conditions.get('time_start')
        time_end = conditions.get('time_end')
        comparison_time = conditions.get('comparison_time')
        brand_keywords = conditions.get('brand_keywords', [])
        model_keywords = conditions.get('model_keywords', [])

        if time_start or time_end or comparison_time:
            filtered = filtered.copy()
            filtered['time_prefix'] = filtered['data_time'].str[:7]

            time_mask = pd.Series([False] * len(filtered), index=filtered.index)

            # å¦‚æœæœ‰å¼€å§‹å’Œç»“æŸæ—¶é—´ï¼Œä½¿ç”¨èŒƒå›´æŸ¥è¯¢
            if time_start and time_end:
                time_mask |= (filtered['time_prefix'] >= time_start) & (filtered['time_prefix'] <= time_end)
            elif time_start:
                time_mask |= (filtered['time_prefix'] == time_start)
            elif time_end:
                time_mask |= (filtered['time_prefix'] == time_end)

            # å¯¹æ¯”æœŸå•ç‹¬å¤„ç†
            if comparison_time:
                time_mask |= (filtered['time_prefix'] == comparison_time)

            filtered = filtered[time_mask]
            filtered = filtered.drop(columns=['time_prefix'])

        print(f"ğŸ“Š æ—¶é—´ç­›é€‰ç»“æœ: {len(filtered)}è¡Œ")

        # 2. å“ç‰Œ/è½¦å‹å…³é”®è¯è¿‡æ»¤
        if brand_keywords or model_keywords:
            keyword_mask = pd.Series([True] * len(filtered), index=filtered.index)

            # å“ç‰Œå…³é”®è¯è¿‡æ»¤
            if brand_keywords:
                brand_mask = pd.Series([False] * len(filtered), index=filtered.index)
                for keyword in brand_keywords:
                    brand_mask |= filtered['display_name'].str.contains(keyword, na=False)
                keyword_mask &= brand_mask

            # è½¦å‹å…³é”®è¯è¿‡æ»¤
            if model_keywords:
                model_mask = pd.Series([False] * len(filtered), index=filtered.index)
                for keyword in model_keywords:
                    model_mask |= filtered['display_name'].str.contains(keyword, na=False)
                keyword_mask &= model_mask

            filtered = filtered[keyword_mask]
            print(f"ğŸ“Š å…³é”®è¯ç­›é€‰ç»“æœ: {len(filtered)}è¡Œ")

        return filtered
    
    def query(self, question: str, verbose: bool = True):
        """
        æŸ¥è¯¢æ–¹æ³•
        :param question: é—®é¢˜æ–‡æœ¬
        :param verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡ºï¼ˆæµ‹è¯•æ—¶è®¾ä¸ºFalseï¼‰
        :return: ç»“æ„åŒ–ç»“æœå­—å…¸
        """
        if verbose:
            print(f"\n{'='*80}")
            print(f"é—®é¢˜: {question}")
            print(f"{'='*80}")
        
        # é‡è¯•æœºåˆ¶ï¼šæœ€å¤š3æ¬¡
        max_retries = 3
        filtered_data = None
        conditions_history = []
        no_data_history = []  # è®°å½•å“ªäº›æ—¶é—´æ²¡æœ‰æ•°æ®

        for attempt in range(max_retries):
            # 1. LLMç”Ÿæˆè¿‡æ»¤æ¡ä»¶
            if verbose and attempt > 0:
                print(f"\nğŸ”„ é‡è¯• {attempt}/{max_retries-1}...")

            # å¦‚æœä¹‹å‰æ²¡æœ‰æ‰¾åˆ°æ•°æ®ï¼Œåœ¨promptä¸­åé¦ˆ
            if attempt > 0 and no_data_history:
                print(f"âš ï¸  æç¤ºï¼šä»¥ä¸‹æ—¶é—´æ²¡æœ‰æ•°æ®ï¼š{', '.join(no_data_history)}")

            conditions = self._generate_filter_conditions(question, no_data_history)
            conditions_history.append(conditions)

            # 2. è¿‡æ»¤æ•°æ®
            filtered_data = self._filter_data(conditions)

            if len(filtered_data) > 0:
                break
            else:
                # è®°å½•æ²¡æœ‰æ•°æ®çš„æ—¶é—´èŒƒå›´
                time_range = conditions.get('time_start') or conditions.get('time_end')
                if time_range:
                    no_data_history.append(time_range)

        # æ„å»ºç»“æœ
        result = {
            "question": question,
            "success": False,
            "answer": None,
            "filtered_rows": 0,
            "conditions": None,
            "error": None,
            "tokens": None,
            "duration_ms": None
        }

        if filtered_data is None or len(filtered_data) == 0:
            result["success"] = True  # æ”¹ä¸ºTrueï¼Œå› ä¸ºè¿™æ˜¯æ­£å¸¸æƒ…å†µ
            result["answer"] = "ã€ç­”æ¡ˆï¼šnullã€‘"  # ç›´æ¥è¿”å›null
            result["conditions"] = conditions_history[-1]
            
            if verbose:
                print("\nâš ï¸  æœªæ‰¾åˆ°æ•°æ®")
                print("ã€ç­”æ¡ˆï¼šnullã€‘")
            
            return result
        
        result["filtered_rows"] = len(filtered_data)
        result["conditions"] = conditions_history[-1]
        
        # 3. LLMåˆ†ææ•°æ®
        context_text = filtered_data.to_string()

        prompt = f"""åŸºäºä»¥ä¸‹æ•°æ®å›ç­”é—®é¢˜ã€‚

ã€æ•°æ®è¯´æ˜ã€‘
- display_nameæ ¼å¼ï¼š"æŒ‡æ ‡ç±»å‹ï¼šå“ç‰Œ_è½¦å‹ï¼šé¢‘ç‡"æˆ–"æŒ‡æ ‡ç±»å‹ï¼šå“ç‰Œ_è½¦å‹Aã€è½¦å‹Bï¼šé¢‘ç‡"
  * ç¤ºä¾‹ï¼š"ä¹˜ç”¨è½¦é”€é‡å¸‚åœºä»½é¢ï¼šå›½å†…åˆ¶é€ +CKD_1.0Lä»¥ä¸‹ï¼šæœˆ"ï¼ˆå¸‚åœºä»½é¢ç±»ï¼‰
  * ç¤ºä¾‹ï¼š"ä¹˜ç”¨è½¦é”€é‡ï¼šä¸Šæ±½é€šç”¨_å‡¯è¿ªæ‹‰å…‹CT5ã€CT6ï¼šæœˆ"ï¼ˆé”€é‡ç±»ï¼‰
  * ç¤ºä¾‹ï¼š"ä¹˜ç”¨è½¦é”€é‡ï¼šæ¯”äºšè¿ª_æµ·è±šï¼šæœˆ"ï¼ˆé”€é‡ç±»ï¼‰
  * é”€é‡æ•°æ®ä½¿ç”¨"_"æˆ–"ã€"åˆ†éš”å“ç‰Œå’Œè½¦å‹
- å“ç‰Œ+è½¦å‹æ„æˆå®Œæ•´æè¿°
- unitå¯èƒ½æ˜¯"%"æˆ–"è¾†"ç­‰

ã€ä½ çš„ä»»åŠ¡ã€‘
1. **å…ˆè¯†åˆ«é—®é¢˜ä¸­è¯¢é—®çš„å“ç‰Œå’Œè½¦å‹**
2. **ä»æ•°æ®ä¸­ç­›é€‰å‡ºæ‰€æœ‰åŒ¹é…çš„è®°å½•**ï¼ˆåŸºäºdisplay_nameä¸­çš„å“ç‰Œå’Œè½¦å‹ï¼‰
3. **æŒ‰ç…§è®¡ç®—è§„åˆ™è®¡ç®—ç­”æ¡ˆ**

ã€æ•°æ®ç­›é€‰è§„åˆ™ã€‘
- å¦‚æœé—®é¢˜é—®"ä¸€æ±½å¤§ä¼—æ½å¢ƒ"ï¼Œéœ€è¦ç­›é€‰display_nameåŒ…å«"ä¸€æ±½å¤§ä¼—"å’Œ"æ½å¢ƒ"çš„è®°å½•
- å¦‚æœé—®é¢˜é—®"æ¯”äºšè¿ªæµ·è±š"ï¼Œéœ€è¦ç­›é€‰display_nameåŒ…å«"æ¯”äºšè¿ª"å’Œ"æµ·è±š"çš„è®°å½•
- å¦‚æœé—®é¢˜é—®å“ç‰Œæ€»é”€é‡ï¼Œéœ€è¦ç­›é€‰è¯¥å“ç‰Œä¸‹æ‰€æœ‰è½¦å‹çš„è®°å½•
- æ³¨æ„ï¼šå“ç‰Œå’Œè½¦å‹å¯èƒ½åœ¨display_nameçš„ä¸åŒä½ç½®ï¼Œéœ€è¦åŒ…å«ä¸¤è€…æ‰åŒ¹é…

ã€è®¡ç®—è§„åˆ™ã€‘
1. åŒæ¯”/ç¯æ¯”å¿…é¡»ç”¨é”€é‡æ•°æ®è®¡ç®—ï¼Œä¸èƒ½ç›´æ¥ä½¿ç”¨"æœˆåŒæ¯”"å­—æ®µ
   - åŒæ¯” = (å½“æœŸé”€é‡æ€»å’Œ - å»å¹´åŒæœŸé”€é‡æ€»å’Œ) / å»å¹´åŒæœŸé”€é‡æ€»å’Œ Ã— 100%
   - ç¯æ¯” = (å½“æœŸé”€é‡æ€»å’Œ - ä¸ŠæœŸé”€é‡æ€»å’Œ) / ä¸ŠæœŸé”€é‡æ€»å’Œ Ã— 100%
   - **åŒæ¯”/ç¯æ¯”ç»“æœå¿…é¡»ä¿ç•™14ä½å°æ•°**
2. å“ç‰Œé”€é‡ = è¯¥å“ç‰Œæ‰€æœ‰è½¦å‹é”€é‡ä¹‹å’Œï¼ˆå“ç‰Œç›¸åŒçš„æ‰€æœ‰è®°å½•ï¼‰
3. åªä½¿ç”¨display_nameåŒ…å«"ï¼šæœˆ"ä¸”unitä¸º"è¾†"çš„é”€é‡æ•°æ®
4. å¦‚æœé—®"å…¨ç³»"æˆ–"æ€»é”€é‡"ï¼Œéœ€è¦æ±‚å’Œæ‰€æœ‰ç›¸å…³è®°å½•

æ•°æ®ï¼š
{context_text}

é—®é¢˜ï¼š{question}

ã€äº¤äº’é£æ ¼è¦æ±‚ã€‘
- **ç®€æ´æ¸…æ™°**ï¼šè¯´è¯è¦ç®€æ´æ¸…æ™°ï¼ŒæŠ“ä½è¦ç‚¹ï¼Œä¸è¦å†—é•¿å•°å—¦
- **ä¸»åŠ¨å‘ŠçŸ¥**ï¼šè°ƒç”¨å·¥å…·æ—¶è¦å‘Šè¯‰ç”¨æˆ·ä½ åœ¨åšä»€ä¹ˆï¼Œè®©ç”¨æˆ·äº†è§£ä½ çš„è¡ŒåŠ¨å’Œæ€è€ƒè¿‡ç¨‹
- **åŠæ—¶åé¦ˆ**ï¼šæ‰§è¡Œè€—æ—¶æ“ä½œå‰è¯´æ˜"æ­£åœ¨åˆ†æ..."ã€"æ­£åœ¨æŸ¥è¯¢..."ç­‰
- **æ ¼å¼è§„èŒƒ**ï¼šé¿å…ä½¿ç”¨*å·ï¼Œç”¨-å·è¡¨ç¤ºåˆ—è¡¨é¡¹ï¼Œä¿æŒè¾“å‡ºæ•´æ´ä¸“ä¸š
- **æ•°å€¼ç­”æ¡ˆæ ¼å¼**ï¼šå›ç­”é”€é‡ã€æ•°é‡ç­‰æ•°å€¼é—®é¢˜æ—¶ï¼Œå¯¹æ¯ä¸ªé—®é¢˜å•ç‹¬è¾“å‡ºæ ¼å¼ï¼šã€ç­”æ¡ˆï¼šæ•°å­—ã€‘

ã€âš ï¸ æœ€ç»ˆç­”æ¡ˆæ ¼å¼è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰ã€‘
**æ‰€æœ‰æ•°å€¼æŸ¥è¯¢é—®é¢˜ï¼Œå¿…é¡»åœ¨å›ç­”æœ«å°¾å•ç‹¬ä¸€è¡Œè¾“å‡ºæ ‡å‡†ç­”æ¡ˆæ ¼å¼ï¼š**

æ ¼å¼ï¼šã€ç­”æ¡ˆï¼šå…·ä½“æ•°å€¼ã€‘

ç¤ºä¾‹ï¼š
- å•ä¸ªæ•°å€¼ï¼šã€ç­”æ¡ˆï¼š4045ã€‘
- ç™¾åˆ†æ¯”ï¼ˆ14ä½å°æ•°ï¼‰ï¼šã€ç­”æ¡ˆï¼š-37.61942154168302%ã€‘
- å¸¦å•ä½ï¼šã€ç­”æ¡ˆï¼š4045 è¾†ã€‘
- æ’åç»“æœï¼šã€ç­”æ¡ˆï¼šå›½å†…åˆ¶é€ , 1349499 è¾†ã€‘
- æœˆåº¦æ•°æ®ï¼šã€ç­”æ¡ˆï¼š1æœˆ: 4890è¾†; 2æœˆ: 3217è¾†; 3æœˆ: 7370è¾†ã€‘
- æ•°æ®ä¸å­˜åœ¨ï¼šã€ç­”æ¡ˆï¼šnullã€‘

**â— ç»å¯¹ç¦æ­¢ç¼–é€ æ•°æ®ï¼š**
- ç­”æ¡ˆå¿…é¡»100%æ¥è‡ªSQLæŸ¥è¯¢ç»“æœ
- å¦‚æœSQLç»“æœä¸­æ²¡æœ‰æŸä¸ªæœˆä»½çš„æ•°æ®ï¼Œç­”æ¡ˆä¸­ä¹Ÿä¸èƒ½åŒ…å«è¯¥æœˆä»½
- ç»ä¸å…è®¸æ¨æµ‹ã€ä¼°ç®—æˆ–ç¼–é€ ä»»ä½•æ•°å€¼
- å¦‚æœæ•°æ®ç¼ºå¤±ï¼Œå¿…é¡»åœ¨ç­”æ¡ˆä¸­æ˜ç¡®æ ‡æ³¨ä¸ºnullæˆ–ç¼ºå¤±"""
        
        if verbose:
            print(f"ğŸ¤– LLMç”Ÿæˆç­”æ¡ˆ...")

        import time
        start_time = time.time()

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )

        answer = response.choices[0].message.content
        usage = response.usage

        duration_ms = (time.time() - start_time) * 1000

        if verbose:
            print(f"\nğŸ“Š Tokenæ¶ˆè€—:")
            print(f"   è¾“å…¥: {usage.prompt_tokens}")
            print(f"   è¾“å‡º: {usage.completion_tokens}")
            print(f"   æ€»è®¡: {usage.total_tokens}")
            print(f"\n{'='*80}")
            print(f"ç­”æ¡ˆ:")
            print(f"{'='*80}")
            print(answer)
            print(f"{'='*80}\n")

        # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦ä¸ºnullï¼ˆæ–°æ ¼å¼ï¼šã€ç­”æ¡ˆï¼šnullã€‘ï¼‰
        if "ã€ç­”æ¡ˆï¼šnullã€‘" in answer or "ã€ç­”æ¡ˆï¼š nullã€‘" in answer:
            result["success"] = False
            result["answer"] = answer
            result["error"] = "LLMæœªèƒ½ä»ç­›é€‰åçš„æ•°æ®ä¸­æ‰¾åˆ°åŒ¹é…çš„å“ç‰Œ/è½¦å‹"
            # æä¾›è¯¦ç»†åˆ†æ
            if verbose:
                print("âš ï¸  LLMæ— æ³•ä»æ•°æ®ä¸­æ‰¾åˆ°åŒ¹é…é¡¹ï¼Œæ­£åœ¨åˆ†æåŸå› ...")
                analysis = self._analyze_data_availability(question, filtered_data)
                result["error_analysis"] = analysis
                print(f"\nåˆ†æç»“æœï¼š\n{analysis}")
        else:
            result["success"] = True
            result["answer"] = answer

        result["tokens"] = {
            "prompt": usage.prompt_tokens,
            "completion": usage.completion_tokens,
            "total": usage.total_tokens
        }
        result["duration_ms"] = round(duration_ms, 2)

        return result

    def _analyze_no_data(self, question: str, no_data_history: list) -> str:
        """
        åˆ†æä¸ºä»€ä¹ˆæ²¡æœ‰æ‰¾åˆ°æ•°æ®
        :param question: ç”¨æˆ·é—®é¢˜
        :param no_data_history: æ²¡æœ‰æ•°æ®çš„æ—¶é—´åˆ—è¡¨
        :return: LLMåˆ†æç»“æœ
        """
        prompt = f"""ç”¨æˆ·é—®äº†ä¸€ä¸ªå…³äºæ±½è½¦é”€é‡çš„é—®é¢˜ï¼Œä½†æ˜¯æˆ‘ä»¬æ²¡æœ‰æ‰¾åˆ°æ•°æ®ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€æœªæ‰¾åˆ°æ•°æ®çš„æ—¶é—´èŒƒå›´ã€‘
{', '.join(no_data_history)}

ã€æ•°æ®æ¦‚å†µã€‘
- æ€»è¡Œæ•°: {len(self.df)}
- æ•°æ®æ—¶é—´èŒƒå›´: {self.df['data_time'].min()} ~ {self.df['data_time'].max()}
- å¯ç”¨å“ç‰Œç¤ºä¾‹: {', '.join(self.df['display_name'].str.split('+').str[1].str.split('ï¼š').str[0].unique()[:10].tolist())}

ã€ä½ çš„ä»»åŠ¡ã€‘
åˆ†æä¸ºä»€ä¹ˆæ²¡æœ‰æ‰¾åˆ°æ•°æ®ï¼Œå¹¶ç»™ç”¨æˆ·ä¸€ä¸ªæ¸…æ™°çš„è§£é‡Šã€‚è¯·è€ƒè™‘ä»¥ä¸‹å¯èƒ½çš„åŸå› ï¼š
1. é—®é¢˜ä¸­æåˆ°çš„æ—¶é—´åœ¨æ•°æ®èŒƒå›´å†…å—ï¼Ÿ
2. é—®é¢˜ä¸­æåˆ°çš„å“ç‰Œ/è½¦å‹åœ¨æ•°æ®ä¸­å­˜åœ¨å—ï¼Ÿ
3. æ˜¯å¦æœ‰æ‹¼å†™é”™è¯¯æˆ–è¡¨è¿°æ–¹å¼ä¸åŒï¼Ÿ
4. æ˜¯å¦éœ€è¦æ›´å®½æ³›çš„æ—¶é—´èŒƒå›´ï¼Ÿ

è¯·ç”¨ç®€æ´ã€å‹å¥½çš„è¯­è¨€è§£é‡ŠåŸå› ï¼Œå¹¶æä¾›å»ºè®®ã€‚"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"åˆ†æå¤±è´¥: {str(e)}"

    def _analyze_data_availability(self, question: str, filtered_data: pd.DataFrame) -> str:
        """
        åˆ†æç­›é€‰åçš„æ•°æ®ä¸­ä¸ºä»€ä¹ˆæ²¡æœ‰åŒ¹é…çš„å“ç‰Œ/è½¦å‹
        :param question: ç”¨æˆ·é—®é¢˜
        :param filtered_data: ç­›é€‰åçš„æ•°æ®
        :return: LLMåˆ†æç»“æœ
        """
        # æå–æ•°æ®ä¸­çš„å“ç‰Œå’Œè½¦å‹
        display_names = filtered_data['display_name'].unique()

        prompt = f"""ç”¨æˆ·é—®äº†ä¸€ä¸ªå…³äºæ±½è½¦é”€é‡çš„é—®é¢˜ã€‚æˆ‘ä»¬å·²ç»æŒ‰æ—¶é—´ç­›é€‰äº†æ•°æ®ï¼Œä½†æ˜¯LLMä»ç„¶æ— æ³•ä»è¿™äº›æ•°æ®ä¸­æ‰¾åˆ°åŒ¹é…çš„å“ç‰Œ/è½¦å‹ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€ç­›é€‰åçš„æ•°æ®ï¼ˆå…±{len(filtered_data)}è¡Œï¼‰ã€‘
å‰20è¡Œçš„display_name:
{chr(10).join(display_names[:20])}

ã€ä½ çš„ä»»åŠ¡ã€‘
åˆ†æä¸ºä»€ä¹ˆæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å“ç‰Œ/è½¦å‹ã€‚è¯·æ£€æŸ¥ï¼š
1. é—®é¢˜ä¸­æåˆ°çš„å“ç‰Œæ˜¯å¦åœ¨æ•°æ®ä¸­ï¼Ÿ
2. é—®é¢˜ä¸­æåˆ°çš„è½¦å‹æ˜¯å¦åœ¨æ•°æ®ä¸­ï¼Ÿ
3. æ˜¯å¦æœ‰è¡¨è¿°æ–¹å¼çš„å·®å¼‚ï¼ˆå¦‚"ä¸€æ±½å¤§ä¼—" vs "ä¸€æ±½å¤§ä¼—+"ï¼‰ï¼Ÿ
4. æ•°æ®ä¸­çš„display_nameæ ¼å¼æ˜¯å¦ç†è§£æ­£ç¡®ï¼Ÿ

è¯·ç»™å‡ºæ¸…æ™°çš„åˆ†æï¼Œå¹¶å»ºè®®å¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"åˆ†æå¤±è´¥: {str(e)}"


def main():
    # æ”¯æŒä»ä»»æ„ç›®å½•è¿è¡Œ
    csv_name = os.getenv('BASELINE_CSV_PATH', 'æ•°æ®æº_é”€é‡.csv')
    script_dir = os.path.dirname(os.path.abspath(__file__))
    csv_path = os.path.join(script_dir, csv_name)
    
    if not os.path.exists(csv_path):
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return
    
    print("\n" + "="*80)
    print("ğŸš€ Baseline Agent Enhanced - LLMç”Ÿæˆè¿‡æ»¤æ¡ä»¶")
    print("="*80)
    print("æµç¨‹ï¼šLLMç”Ÿæˆè¿‡æ»¤æ¡ä»¶ â†’ pandasè¿‡æ»¤ â†’ LLMåˆ†æ")
    print("="*80 + "\n")
    
    agent = EnhancedBaselineAgent(csv_path)
    
    while True:
        try:
            question = input("\nğŸ’¬ è¯·è¾“å…¥é—®é¢˜: ").strip()
            
            if question.lower() in ['/quit', '/exit']:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            if not question:
                continue
            
            agent.query(question)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()
