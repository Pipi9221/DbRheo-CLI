"""
Baseline2 Agent - RAGå‘é‡æ£€ç´¢æ–¹æ¡ˆ
ä½¿ç”¨å‘é‡æ•°æ®åº“è¿›è¡Œè¯­ä¹‰æ£€ç´¢ï¼Œæå‡æ£€ç´¢å‡†ç¡®ç‡
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../packages/core/src'))

import pandas as pd
import numpy as np
from openai import OpenAI
from typing import List, Dict
import re
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
env_file = os.path.abspath(os.path.join(os.path.dirname(__file__), '../.env'))
print(f"âœ“ åŠ è½½ç¯å¢ƒå˜é‡: {env_file}")
load_dotenv(env_file)

class BaselineAgentRAG:
    """
    Baseline2 - RAGå‘é‡æ£€ç´¢æ–¹æ¡ˆ
    
    æ”¹è¿›ï¼š
    1. æ•°æ®é¢„å¤„ç†ï¼šCSVè½¬è‡ªç„¶è¯­è¨€
    2. å‘é‡åŒ–ï¼šä½¿ç”¨embeddingæ¨¡å‹
    3. è¯­ä¹‰æ£€ç´¢ï¼šå‘é‡ç›¸ä¼¼åº¦åŒ¹é…
    """
    
    def __init__(self, csv_path: str):
        self.csv_path = csv_path
        self.df = pd.read_csv(csv_path)

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.client = OpenAI(
            api_key=os.getenv('BASELINE_OPENAI_API_KEY'),
            base_url=os.getenv('BASELINE_OPENAI_API_BASE')
        )
        self.model = os.getenv('BASELINE_MODEL', 'qwen-flash')

        print(f"âœ… åŠ è½½CSV: {len(self.df)}è¡Œ")
        print(f"âœ… ä½¿ç”¨æ¨¡å‹: {self.model}")

        # æ•°æ®é¢„å¤„ç†
        self.texts, self.metadata = self._preprocess_data()
        print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ: {len(self.texts)}æ¡æ–‡æœ¬")

        # æ„å»ºå‘é‡åº“
        self.vectors = self._build_vectors()
        print(f"âœ… å‘é‡åº“æ„å»ºå®Œæˆ")
    
    def _preprocess_data(self) -> tuple:
        """æ•°æ®é¢„å¤„ç†ï¼šCSVè½¬è‡ªç„¶è¯­è¨€"""
        texts = []
        metadata = []
        
        for idx, row in self.df.iterrows():
            # è§£ædisplay_nameæå–å“ç‰Œå’Œè½¦å‹
            display = row['display_name']
            
            # æå–å“ç‰Œ_è½¦å‹
            if 'ï¼š' in display and '_' in display:
                parts = display.split('ï¼š')
                if len(parts) >= 2:
                    brand_model = parts[1].split('ï¼š')[0]
                    
                    # è½¬æ¢æˆè‡ªç„¶è¯­è¨€
                    date = row['data_time'][:7]  # 2012-12-01 -> 2012-12
                    value = row['ind_value']
                    unit = row['unit']
                    
                    text = f"{date}ï¼Œ{brand_model}çš„é”€é‡ä¸º{value}{unit}"
                    texts.append(text)
                    metadata.append(row.to_dict())
        
        return texts, metadata
    
    def _build_vectors(self) -> np.ndarray:
        """æ„å»ºå‘é‡åº“ï¼ˆä½¿ç”¨OpenAI embeddingï¼‰"""
        print("ğŸ”„ æ­£åœ¨ç”Ÿæˆå‘é‡...")
        
        vectors = []
        batch_size = 25  # é€šä¹‰åƒé—®é™åˆ¶æœ€å¤š25æ¡
        
        for i in range(0, len(self.texts), batch_size):
            batch = self.texts[i:i+batch_size]
            
            try:
                response = self.client.embeddings.create(
                    model="text-embedding-v1",
                    input=batch
                )
                
                batch_vectors = [item.embedding for item in response.data]
                vectors.extend(batch_vectors)
                
                print(f"   è¿›åº¦: {min(i+batch_size, len(self.texts))}/{len(self.texts)}")
                
            except Exception as e:
                print(f"âš ï¸  å‘é‡åŒ–å¤±è´¥: {e}")
                # ä½¿ç”¨ç®€å•çš„TF-IDFä½œä¸ºfallback
                return self._build_tfidf_vectors()
        
        return np.array(vectors)
    
    def _build_tfidf_vectors(self) -> np.ndarray:
        """Fallback: ä½¿ç”¨TF-IDF"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        print("âš ï¸  ä½¿ç”¨TF-IDFä½œä¸ºfallback")
        vectorizer = TfidfVectorizer(max_features=512)
        vectors = vectorizer.fit_transform(self.texts).toarray()
        self.vectorizer = vectorizer
        return vectors
    
    def _retrieve(self, question: str, top_k: int = 20) -> List[Dict]:
        """å‘é‡æ£€ç´¢"""
        # é—®é¢˜å‘é‡åŒ–
        try:
            response = self.client.embeddings.create(
                model="text-embedding-v1",
                input=[question]
            )
            query_vector = np.array(response.data[0].embedding)
        except:
            # Fallback to TF-IDF
            if hasattr(self, 'vectorizer'):
                query_vector = self.vectorizer.transform([question]).toarray()[0]
            else:
                return []
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(self.vectors, query_vector)
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        # è¿”å›ç»“æœ
        results = []
        for idx in top_indices:
            results.append({
                'text': self.texts[idx],
                'metadata': self.metadata[idx],
                'score': float(similarities[idx])
            })
        
        print(f"ğŸ“Š æ£€ç´¢åˆ°{len(results)}æ¡ç›¸å…³æ•°æ®")
        for i, r in enumerate(results[:3]):
            print(f"   {i+1}. {r['text'][:50]}... (å¾—åˆ†: {r['score']:.3f})")
        
        return results
    
    def query(self, question: str) -> str:
        """å¤„ç†æŸ¥è¯¢"""
        print(f"\n{'='*80}")
        print(f"é—®é¢˜: {question}")
        print(f"{'='*80}")
        
        # å‘é‡æ£€ç´¢
        results = self._retrieve(question, top_k=20)
        
        if not results:
            return "âŒ æœªæ‰¾åˆ°ç›¸å…³æ•°æ®"
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_data = pd.DataFrame([r['metadata'] for r in results])
        context_text = context_data.to_string()
        
        # è°ƒç”¨LLM
        prompt = f"""åŸºäºä»¥ä¸‹æ•°æ®å›ç­”é—®é¢˜ã€‚

æ•°æ®ï¼š
{context_text}

é—®é¢˜ï¼š{question}

è¯·ç»™å‡ºå‡†ç¡®ç­”æ¡ˆã€‚"""
        
        print(f"ğŸ¤– è°ƒç”¨LLM...")
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            
            answer = response.choices[0].message.content
            print(f"\n{'='*80}")
            print(f"LLMå›ç­”:")
            print(f"{'='*80}")
            print(answer)
            print(f"{'='*80}\n")
            
            return answer
            
        except Exception as e:
            return f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}"


def main():
    """è¿è¡ŒBaseline2"""
    # CSVæ–‡ä»¶è·¯å¾„ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    csv_name = os.getenv('BASELINE_CSV_PATH', 'æ•°æ®æº_é”€é‡.csv')
    script_dir = os.path.dirname(os.path.abspath(__file__))
    csv_path = os.path.join(script_dir, csv_name)
    
    if not os.path.exists(csv_path):
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return
    
    print("\n" + "="*80)
    print("ğŸš€ Baseline2 Agent - RAGå‘é‡æ£€ç´¢æ–¹æ¡ˆ")
    print("="*80)
    print("ä½¿ç”¨å‘é‡æ•°æ®åº“è¿›è¡Œè¯­ä¹‰æ£€ç´¢")
    print("è¾“å…¥ /quit é€€å‡º")
    print("="*80 + "\n")
    
    agent = BaselineAgentRAG(csv_path)
    
    while True:
        try:
            question = input("\nğŸ’¬ è¯·è¾“å…¥é—®é¢˜: ").strip()
            
            if question.lower() in ['/quit', '/exit']:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            if not question:
                continue
            
            agent.query(question)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
