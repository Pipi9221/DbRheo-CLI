"""
åŸºçº¿Agent - æ¨¡æ‹Ÿé€šä¹‰åƒé—®ç­‰å¹³å°å¤„ç†å¤§æ–‡ä»¶çš„æ–¹å¼
ä½¿ç”¨åˆ†å—+æ£€ç´¢ç­–ç•¥ï¼Œå±•ç¤ºçº¯LLMæ–¹æ¡ˆçš„å±€é™æ€§
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../packages/core/src'))

import pandas as pd
from openai import OpenAI
from typing import List, Dict
import re
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
env_file = os.path.abspath(os.path.join(os.path.dirname(__file__), '../.env'))
print(f"âœ“ åŠ è½½ç¯å¢ƒå˜é‡: {env_file}")
load_dotenv(env_file)

class BaselineAgent:
    """
    åŸºçº¿Agent - æ¨¡æ‹Ÿä¸»æµAIå¹³å°çš„æ–‡ä»¶å¤„ç†æ–¹å¼
    
    ç­–ç•¥ï¼š
    1. å°†CSVæ–‡ä»¶åˆ†å—ï¼ˆæ¯å—100-200è¡Œï¼‰
    2. æ ¹æ®ç”¨æˆ·é—®é¢˜æ£€ç´¢ç›¸å…³å—ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰
    3. å°†ç›¸å…³å—æä¾›ç»™LLMè¿›è¡Œåˆ†æå’Œè®¡ç®—
    
    å±€é™æ€§ï¼š
    - å¯èƒ½é—æ¼æ•°æ®ï¼ˆæ£€ç´¢ä¸å‡†ç¡®ï¼‰
    - è®¡ç®—å¯èƒ½å‡ºé”™ï¼ˆLLMè®¡ç®—èƒ½åŠ›æœ‰é™ï¼‰
    - ä¸ç¨³å®šï¼ˆæ¯æ¬¡ç»“æœå¯èƒ½ä¸åŒï¼‰
    """
    
    def __init__(self, csv_path: str, chunk_size: int = None):
        """
        åˆå§‹åŒ–åŸºçº¿Agent

        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„
            chunk_size: æ¯ä¸ªæ•°æ®å—çš„è¡Œæ•°ï¼ˆNoneåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        """
        self.csv_path = csv_path
        self.chunk_size = chunk_size or int(os.getenv('BASELINE_CHUNK_SIZE', '150'))
        self.df = pd.read_csv(csv_path)
        self.chunks = self._create_chunks()

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨é€šä¹‰åƒé—®ï¼‰
        self.client = OpenAI(
            api_key=os.getenv('BASELINE_OPENAI_API_KEY'),
            base_url=os.getenv('BASELINE_OPENAI_API_BASE')
        )
        self.model = os.getenv('BASELINE_MODEL', 'qwen-flash')

        print(f"âœ… åŠ è½½CSVæ–‡ä»¶: {len(self.df)}è¡Œæ•°æ®")
        print(f"âœ… åˆ†å—ç­–ç•¥: æ¯å—{self.chunk_size}è¡Œï¼Œå…±{len(self.chunks)}å—")
        print(f"âœ… ä½¿ç”¨æ¨¡å‹: {self.model}")
    
    def _create_chunks(self) -> List[Dict]:
        """å°†æ•°æ®åˆ†å—"""
        chunks = []
        for i in range(0, len(self.df), self.chunk_size):
            chunk_df = self.df.iloc[i:i+self.chunk_size]
            chunks.append({
                'id': len(chunks),
                'start_row': i,
                'end_row': min(i+self.chunk_size, len(self.df)),
                'data': chunk_df,
                'text': chunk_df.to_string()
            })
        return chunks
    
    def _retrieve_relevant_chunks(self, question: str, top_k: int = 10) -> List[Dict]:
        """
        æ”¹è¿›çš„æ£€ç´¢ç­–ç•¥ï¼šæ›´å¤šå…³é”®è¯+æ›´å¤šå—
        """
        # æå–æ‰€æœ‰å¯èƒ½çš„å…³é”®è¯
        keywords = []
        
        # ä»é—®é¢˜ä¸­æå–å“ç‰Œå’Œè½¦å‹ï¼ˆæ›´æ™ºèƒ½ï¼‰
        # åˆ†è¯æå–
        words = re.findall(r'[\u4e00-\u9fa5a-zA-Z0-9]+', question)
        for word in words:
            if len(word) >= 2:  # è‡³å°‘2ä¸ªå­—ç¬¦
                keywords.append(word)
        
        # å¹´ä»½å’Œæœˆä»½
        years = re.findall(r'20\d{2}', question)
        keywords.extend(years)
        months = re.findall(r'(\d{1,2})æœˆ', question)
        keywords.extend([f"{m}æœˆ" for m in months])
        
        print(f"ğŸ” æ£€ç´¢å…³é”®è¯: {keywords}")
        
        # è®¡ç®—æ¯ä¸ªå—çš„ç›¸å…³æ€§å¾—åˆ†
        scored_chunks = []
        for chunk in self.chunks:
            score = 0
            chunk_text = chunk['text']
            for keyword in keywords:
                # å…³é”®è¯åŒ¹é…å¾—åˆ†
                count = chunk_text.count(str(keyword))
                if count > 0:
                    score += count * len(keyword)  # é•¿å…³é”®è¯æƒé‡æ›´é«˜
            
            if score > 0:
                scored_chunks.append((score, chunk))
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›æ‰€æœ‰å—ï¼ˆå…œåº•ç­–ç•¥ï¼‰
        if not scored_chunks:
            print(f"âš ï¸  æœªæ‰¾åˆ°åŒ¹é…å—ï¼Œä½¿ç”¨æ‰€æœ‰æ•°æ®")
            return self.chunks
        
        # æŒ‰å¾—åˆ†æ’åºï¼Œå–top_k
        scored_chunks.sort(reverse=True, key=lambda x: x[0])
        relevant_chunks = [chunk for score, chunk in scored_chunks[:top_k]]
        
        print(f"ğŸ“Š æ£€ç´¢åˆ°{len(relevant_chunks)}ä¸ªç›¸å…³æ•°æ®å—")
        for chunk in relevant_chunks[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"   - å—{chunk['id']}: ç¬¬{chunk['start_row']}-{chunk['end_row']}è¡Œ")
        if len(relevant_chunks) > 3:
            print(f"   ... è¿˜æœ‰{len(relevant_chunks)-3}ä¸ªå—")
        
        return relevant_chunks
    
    def query(self, question: str) -> str:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            LLMçš„å›ç­”
        """
        print(f"\n{'='*80}")
        print(f"é—®é¢˜: {question}")
        print(f"{'='*80}")
        
        # 1. æ£€ç´¢ç›¸å…³æ•°æ®å—
        relevant_chunks = self._retrieve_relevant_chunks(question, top_k=10)
        
        if not relevant_chunks:
            return "âŒ æœªæ‰¾åˆ°ç›¸å…³æ•°æ®"
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡ï¼ˆå°†ç›¸å…³æ•°æ®å—åˆå¹¶ï¼‰
        context_data = pd.concat([chunk['data'] for chunk in relevant_chunks])
        context_text = context_data.to_string()
        
        print(f"ğŸ“ ä¸Šä¸‹æ–‡æ•°æ®: {len(context_data)}è¡Œ")
        
        # 3. æ„å»ºæç¤ºè¯
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æåŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹CSVæ•°æ®å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æ•°æ®æ ¼å¼ï¼šå“ç‰Œ | è½¦å‹ | æŒ‡æ ‡åç§° | æ—¥æœŸ | æ•°å€¼

æ•°æ®å†…å®¹ï¼š
{context_text}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·ä»”ç»†åˆ†ææ•°æ®ï¼Œè¿›è¡Œå¿…è¦çš„è®¡ç®—ï¼Œå¹¶ç»™å‡ºå‡†ç¡®ç­”æ¡ˆã€‚å¦‚æœéœ€è¦æ±‡æ€»ã€åŒæ¯”ã€ç¯æ¯”ç­‰è®¡ç®—ï¼Œè¯·æ˜ç¡®åˆ—å‡ºè®¡ç®—è¿‡ç¨‹ã€‚"""
        
        # 4. è°ƒç”¨LLM
        print(f"ğŸ¤– è°ƒç”¨LLM...")
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1  # é™ä½æ¸©åº¦ä»¥æé«˜ç¨³å®šæ€§
            )
            
            answer = response.choices[0].message.content
            print(f"\n{'='*80}")
            print(f"LLMå›ç­”:")
            print(f"{'='*80}")
            print(answer)
            print(f"{'='*80}\n")
            
            return answer
            
        except Exception as e:
            return f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}"


def main():
    """è¿è¡ŒåŸºçº¿Agent - äº¤äº’å¼CLI"""

    # CSVæ–‡ä»¶è·¯å¾„ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    csv_name = os.getenv('BASELINE_CSV_PATH', 'æ•°æ®æº_é”€é‡.csv')
    script_dir = os.path.dirname(os.path.abspath(__file__))
    csv_path = os.path.join(script_dir, csv_name)
    
    # æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(csv_path):
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return
    
    # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
    print("\n" + "="*80)
    print("ğŸ”¬ Baseline Agent - çº¯LLMæ–¹æ¡ˆï¼ˆæ–¹æ¡ˆAï¼‰")
    print("="*80)
    print("æ¨¡æ‹Ÿé€šä¹‰åƒé—®ç­‰å¹³å°çš„æ–‡ä»¶å¤„ç†æ–¹å¼ï¼šåˆ†å—+æ£€ç´¢+LLMè®¡ç®—")
    print("è¾“å…¥ /quit æˆ– /exit é€€å‡º")
    print("="*80 + "\n")
    
    # åˆå§‹åŒ–åŸºçº¿Agent
    agent = BaselineAgent(csv_path, chunk_size=150)
    
    # äº¤äº’å¼å¾ªç¯
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            question = input("\nğŸ’¬ è¯·è¾“å…¥é—®é¢˜: ").strip()
            
            # å¤„ç†é€€å‡ºå‘½ä»¤
            if question.lower() in ['/quit', '/exit', 'quit', 'exit']:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            # è·³è¿‡ç©ºè¾“å…¥
            if not question:
                continue
            
            # å¤„ç†æŸ¥è¯¢
            agent.query(question)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except EOFError:
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
