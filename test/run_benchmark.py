"""
Benchmarkæµ‹è¯•è„šæœ¬ - æ‰¹é‡æµ‹è¯•baseline_agent_enhanced.py
æµ‹è¯•ç»“æœä¿å­˜åˆ°CSVå’ŒJSONæ ¼å¼
"""
import sys
import os
import csv
import json
import time
import re
from datetime import datetime
from pathlib import Path
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
baseline_dir = project_root / "baseline"
sys.path.insert(0, str(baseline_dir))

# å¯¼å…¥agent
from baseline_agent_enhanced import EnhancedBaselineAgent

class BenchmarkTester:
    """Benchmarkæµ‹è¯•å™¨"""
    
    def __init__(self, csv_path: str):
        self.csv_path = csv_path
        self.agent = None
        self.results = []
        
    def initialize_agent(self):
        """åˆå§‹åŒ–agent"""
        print(f"\n{'='*80}")
        print(f"ğŸš€ åˆå§‹åŒ–Agent...")
        print(f"{'='*80}")
        self.agent = EnhancedBaselineAgent(self.csv_path)
        
    def load_benchmark_questions(self, benchmark_csv: str) -> pd.DataFrame:
        """åŠ è½½benchmarké—®é¢˜"""
        print(f"\nğŸ“‚ åŠ è½½benchmarké—®é¢˜: {benchmark_csv}")
        df = pd.read_csv(benchmark_csv)
        print(f"âœ… åŠ è½½ {len(df)} ä¸ªé—®é¢˜")
        print(f"   é—®é¢˜ç±»å‹åˆ†å¸ƒ: {df['Type'].value_counts().to_dict()}")
        return df
    
    def extract_answer_number(self, answer: str) -> float:
        """
        ä»ç­”æ¡ˆä¸­æå–æ•°å­—
        æ–°æ ¼å¼ï¼šã€ç­”æ¡ˆï¼šå…·ä½“æ•°å€¼ã€‘
        æ—§æ ¼å¼ï¼šresult = xxxï¼ˆå…¼å®¹ï¼‰
        """
        if pd.isna(answer) or answer is None:
            return None

        answer = str(answer).strip()

        # ä¼˜å…ˆæå–æ–°æ ¼å¼ï¼šã€ç­”æ¡ˆï¼šxxxã€‘
        new_match = re.search(r'ã€ç­”æ¡ˆï¼š\s*([+-]?\d+\.?\d*)', answer)
        if new_match:
            value_str = new_match.group(1)
            # å¤„ç†null
            if value_str.lower() == 'null':
                return None
            return float(value_str)

        # å…¼å®¹æ—§æ ¼å¼ï¼šresult = xxx
        result_match = re.search(r'result\s*=\s*([+-]?\d+\.?\d*)', answer, re.IGNORECASE)
        if result_match:
            value_str = result_match.group(1)
            # å¤„ç†null
            if value_str.lower() == 'null':
                return None
            return float(value_str)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ ¼å¼ï¼Œå°è¯•æå–null
        if 'ã€ç­”æ¡ˆï¼šnullã€‘' in answer or 'ã€ç­”æ¡ˆï¼š nullã€‘' in answer or 'null' in answer.lower() or 'æ— ' in answer or 'ä¸é€‚ç”¨' in answer:
            return None

        # é™çº§æ–¹æ¡ˆï¼šæå–æ‰€æœ‰æ•°å­—
        numbers = re.findall(r'-?\d+\.?\d*', answer)
        if numbers:
            # è¿‡æ»¤æ‰å¹´ä»½ï¼ˆ1000-2100ï¼‰ï¼Œå–æœ€å¤§å€¼
            num_values = [float(n) for n in numbers]
            filtered_nums = [n for n in num_values if not (1000 <= n <= 2100)]
            if filtered_nums:
                return max(filtered_nums)
            return max(num_values) if num_values else None

        return None
    
    def compare_results(self, predicted: str, expected: str, tolerance: float = 0.01) -> dict:
        """
        æ¯”è¾ƒé¢„æµ‹ç»“æœå’ŒæœŸæœ›ç»“æœ
        :param predicted: é¢„æµ‹ç­”æ¡ˆ
        :param expected: æœŸæœ›ç­”æ¡ˆ
        :param tolerance: å®¹å·®ï¼ˆç™¾åˆ†æ¯”ï¼‰
        :return: æ¯”è¾ƒç»“æœå­—å…¸
        """
        comparison = {
            "match": False,
            "predicted_number": None,
            "expected_number": None,
            "difference": None,
            "difference_percent": None,
            "within_tolerance": False
        }
        
        # æå–æ•°å­—
        pred_num = self.extract_answer_number(predicted)
        
        # æœŸæœ›ç­”æ¡ˆé€šå¸¸æ˜¯"æ•°å­— å•ä½"æ ¼å¼ï¼Œç®€å•æå–ç¬¬ä¸€ä¸ªæ•°å­—
        if pd.isna(expected) or expected is None:
            exp_num = None
        else:
            # æå–æœŸæœ›ç­”æ¡ˆä¸­çš„æ•°å­—ï¼ˆå¦‚ "4045 è¾†" -> 4045ï¼‰
            exp_match = re.search(r'([+-]?\d+\.?\d*)', str(expected))
            if exp_match:
                exp_num = float(exp_match.group(1))
            else:
                exp_num = None
        
        comparison["predicted_number"] = pred_num
        comparison["expected_number"] = exp_num
        
        # å¦‚æœä»»ä¸€æ•°å­—ä¸ºNoneï¼Œæ— æ³•æ¯”è¾ƒ
        if pred_num is None or exp_num is None:
            comparison["match"] = False
            return comparison
        
        # è®¡ç®—å·®å¼‚
        difference = abs(pred_num - exp_num)
        comparison["difference"] = difference
        
        # è®¡ç®—ç™¾åˆ†æ¯”å·®å¼‚
        if exp_num != 0:
            difference_percent = (difference / abs(exp_num)) * 100
            comparison["difference_percent"] = difference_percent
            comparison["within_tolerance"] = difference_percent <= tolerance
        else:
            comparison["difference_percent"] = None
            comparison["within_tolerance"] = difference <= tolerance
        
        # åˆ¤æ–­æ˜¯å¦åŒ¹é…ï¼ˆåœ¨å®¹å·®èŒƒå›´å†…ï¼‰
        comparison["match"] = comparison["within_tolerance"]
        
        return comparison
    
    def run_single_test(self, row: pd.Series, test_num: int, total_tests: int) -> dict:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        question_id = f"Q{test_num}"
        question_type = row['Type']
        question_text = row['Question']
        expected_sql = row['SQL']
        expected_answer = row['Answer']
        
        print(f"\n{'='*80}")
        print(f"ğŸ“ æµ‹è¯• {test_num}/{total_tests} [{question_id}] - Type {question_type}")
        print(f"{'='*80}")
        print(f"é—®é¢˜: {question_text}")
        print(f"æœŸæœ›ç­”æ¡ˆ: {expected_answer}")
        
        # æ‰§è¡ŒæŸ¥è¯¢ï¼ˆverbose=Falseï¼Œå‡å°‘è¾“å‡ºï¼‰
        start_time = time.time()
        result = self.agent.query(question_text, verbose=False)
        execution_time = time.time() - start_time
        
        # æ·»åŠ åŸºç¡€ä¿¡æ¯
        test_result = {
            "test_num": test_num,
            "test_id": question_id,
            "type": question_type,
            "question": question_text,
            "expected_sql": expected_sql,
            "expected_answer": expected_answer,
            "execution_time": round(execution_time, 2),
            "timestamp": datetime.now().isoformat()
        }
        
        # æ·»åŠ agentç»“æœ
        test_result.update({
            "success": result["success"],
            "predicted_answer": result["answer"],
            "filtered_rows": result["filtered_rows"],
            "error": result.get("error"),
            "tokens_input": result.get("tokens", {}).get("prompt") if result.get("tokens") else None,
            "tokens_output": result.get("tokens", {}).get("completion") if result.get("tokens") else None,
            "tokens_total": result.get("tokens", {}).get("total") if result.get("tokens") else None,
            "llm_duration_ms": result.get("duration_ms")
        })
        
        # æ¯”è¾ƒç»“æœ
        if result["success"] and result["answer"]:
            comparison = self.compare_results(result["answer"], expected_answer)
            test_result.update(comparison)
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print(f"\nç»“æœæ‘˜è¦:")
        if test_result["success"]:
            print(f"  âœ… æŸ¥è¯¢æˆåŠŸ")
            print(f"  ğŸ“Š é¢„æµ‹ç­”æ¡ˆ: {result['answer'][:100]}...")
            if "match" in test_result:
                print(f"  ğŸ¯ åŒ¹é…ç»“æœ: {'âœ… åŒ¹é…' if test_result['match'] else 'âŒ ä¸åŒ¹é…'}")
                if test_result["difference_percent"] is not None:
                    print(f"  ğŸ“‰ å·®å¼‚: {test_result['difference_percent']:.2f}%")
            print(f"  â±ï¸ è€—æ—¶: {execution_time:.2f}ç§’")
        else:
            print(f"  âŒ æŸ¥è¯¢å¤±è´¥: {test_result.get('error', 'Unknown error')}")
        
        return test_result
    
    def run_benchmark(self, benchmark_csv: str, output_dir: str = None, max_tests: int = None, start_idx: int = 1):
        """
        è¿è¡Œå®Œæ•´benchmarkæµ‹è¯•
        :param benchmark_csv: benchmark CSVæ–‡ä»¶è·¯å¾„
        :param output_dir: è¾“å‡ºç›®å½•
        :param max_tests: æœ€å¤§æµ‹è¯•æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨æµ‹è¯•ï¼‰
        :param start_idx: èµ·å§‹æµ‹è¯•ç´¢å¼•ï¼ˆä»1å¼€å§‹ï¼‰
        """
        print(f"\n{'='*80}")
        print(f"ğŸš€ å¼€å§‹Benchmarkæµ‹è¯•")
        print(f"{'='*80}")

        # åˆå§‹åŒ–agent
        self.initialize_agent()

        # åŠ è½½é—®é¢˜
        df = self.load_benchmark_questions(benchmark_csv)

        # åº”ç”¨èµ·å§‹ç´¢å¼•
        if start_idx > 1:
            # Python ä½¿ç”¨ 0-based ç´¢å¼•ï¼Œæ‰€ä»¥ start_idx-1
            df = df.iloc[start_idx - 1:].copy()
            df.index = range(1, len(df) + 1)  # é‡ç½®ç´¢å¼•ä»1å¼€å§‹
            print(f"\nâš ï¸  èµ·å§‹ç´¢å¼•: ç¬¬{start_idx}ä¸ªé—®é¢˜")

        # é™åˆ¶æµ‹è¯•æ•°é‡
        if max_tests and max_tests < len(df):
            df = df.head(max_tests)
            print(f"âš ï¸  é™åˆ¶æµ‹è¯•æ•°é‡: {max_tests}")

        total_tests = len(df)
        print(f"\nğŸ“Š è®¡åˆ’æµ‹è¯• {total_tests} ä¸ªé—®é¢˜")
        print(f"   æµ‹è¯•èŒƒå›´: ç¬¬{start_idx}ä¸ª ~ ç¬¬{start_idx + total_tests - 1}ä¸ªé—®é¢˜")

        # è¿è¡Œæµ‹è¯•
        start_time = time.time()
        self.results = []

        for idx, row in df.iterrows():
            test_num = start_idx + idx  # è®¡ç®—å®é™…æµ‹è¯•ç¼–å·
            try:
                test_result = self.run_single_test(row, test_num, total_tests)
                self.results.append(test_result)

                # æ¯10ä¸ªæµ‹è¯•æ˜¾ç¤ºè¿›åº¦
                if test_num % 10 == 0:
                    elapsed = time.time() - start_time
                    avg_time = elapsed / (test_num - start_idx + 1)
                    print(f"\nğŸ“ˆ è¿›åº¦: {test_num}/{start_idx + total_tests - 1} ({(test_num - start_idx + 1)/total_tests*100:.1f}%)")
                    print(f"   å·²ç”¨æ—¶é—´: {elapsed:.1f}ç§’")
                    print(f"   å¹³å‡è€—æ—¶: {avg_time:.2f}ç§’/é¢˜")
                    print(f"   é¢„è®¡å‰©ä½™: {avg_time * (total_tests - (test_num - start_idx + 1)):.1f}ç§’")

            except Exception as e:
                print(f"\nâŒ æµ‹è¯• {test_num} å¤±è´¥: {str(e)}")
                import traceback
                traceback.print_exc()
                # æ·»åŠ å¤±è´¥è®°å½•
                self.results.append({
                    "test_num": test_num,
                    "test_id": f"Q{test_num}",
                    "type": row.get('Type'),
                    "question": row.get('Question'),
                    "expected_answer": row.get('Answer'),
                    "success": False,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                })

        total_time = time.time() - start_time
        print(f"\n{'='*80}")
        print(f"âœ… æµ‹è¯•å®Œæˆ!")
        print(f"{'='*80}")
        print(f"   æ€»è€—æ—¶: {total_time:.1f}ç§’")
        print(f"   å¹³å‡: {total_time/len(self.results):.2f}ç§’/é¢˜")

        # ä¿å­˜ç»“æœ
        if output_dir is None:
            output_dir = str(Path(__file__).parent / "result")

        self.save_results(output_dir)
        self.generate_summary(output_dir)
    
    def save_results(self, output_dir: str):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_dir}")
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä¸ºJSON
        json_file = output_path / "benchmark_results.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"  âœ… JSON: {json_file}")
        
        # ä¿å­˜ä¸ºCSV
        csv_file = output_path / "benchmark_results.csv"
        pd.DataFrame(self.results).to_csv(csv_file, index=False, encoding='utf-8-sig')
        print(f"  âœ… CSV: {csv_file}")
        
        # ä¿å­˜è¯¦ç»†æ—¥å¿—
        log_file = output_path / "benchmark_detailed.log"
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("Benchmarkæµ‹è¯•è¯¦ç»†æ—¥å¿—\n")
            f.write("="*80 + "\n\n")
            
            for result in self.results:
                f.write(f"æµ‹è¯• {result['test_num']} [{result['test_id']}]\n")
                f.write("-"*80 + "\n")
                f.write(f"ç±»å‹: {result['type']}\n")
                f.write(f"é—®é¢˜: {result['question']}\n")
                f.write(f"æœŸæœ›ç­”æ¡ˆ: {result['expected_answer']}\n")
                f.write(f"é¢„æµ‹ç­”æ¡ˆ: {result['predicted_answer']}\n")
                f.write(f"æˆåŠŸ: {'âœ…' if result['success'] else 'âŒ'}\n")
                
                if 'match' in result:
                    f.write(f"åŒ¹é…: {'âœ…' if result['match'] else 'âŒ'}\n")
                    if result['difference_percent'] is not None:
                        f.write(f"å·®å¼‚: {result['difference_percent']:.2f}%\n")
                
                if result['error']:
                    f.write(f"é”™è¯¯: {result['error']}\n")
                
                f.write(f"è€—æ—¶: {result['execution_time']}ç§’\n")
                f.write(f"æ—¶é—´æˆ³: {result['timestamp']}\n\n")
        
        print(f"  âœ… æ—¥å¿—: {log_file}")
    
    def generate_summary(self, output_dir: str):
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦æŠ¥å‘Š"""
        print(f"\nğŸ“Š ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š...")
        
        # ç»Ÿè®¡æ•°æ®
        total_tests = len(self.results)
        successful_tests = sum(1 for r in self.results if r['success'])
        matched_tests = sum(1 for r in self.results if r.get('match', False))
        
        # ç±»å‹ç»Ÿè®¡
        type_stats = {}
        for result in self.results:
            t = result['type']
            if t not in type_stats:
                type_stats[t] = {'total': 0, 'success': 0, 'match': 0}
            type_stats[t]['total'] += 1
            if result['success']:
                type_stats[t]['success'] += 1
            if result.get('match', False):
                type_stats[t]['match'] += 1
        
        # Tokenç»Ÿè®¡
        total_tokens = sum(r.get('tokens_total', 0) for r in self.results)
        avg_tokens = total_tokens / total_tests if total_tests > 0 else 0
        
        # è€—æ—¶ç»Ÿè®¡
        total_time = sum(r['execution_time'] for r in self.results)
        avg_time = total_time / total_tests if total_tests > 0 else 0
        
        # ç”Ÿæˆæ‘˜è¦
        summary = {
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "success_rate": f"{successful_tests/total_tests*100:.2f}%",
            "matched_tests": matched_tests,
            "match_rate": f"{matched_tests/total_tests*100:.2f}%" if total_tests > 0 else "N/A",
            "total_tokens": total_tokens,
            "avg_tokens": f"{avg_tokens:.0f}",
            "total_time": f"{total_time:.1f}s",
            "avg_time": f"{avg_time:.2f}s",
            "type_stats": type_stats,
            "generated_at": datetime.now().isoformat()
        }
        
        # ä¿å­˜æ‘˜è¦
        summary_file = Path(output_dir) / "benchmark_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        md_file = Path(output_dir) / "benchmark_report.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write("# Benchmarkæµ‹è¯•æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## æ€»ä½“ç»Ÿè®¡\n\n")
            f.write(f"| æŒ‡æ ‡ | æ•°å€¼ |\n")
            f.write(f"|------|------|\n")
            f.write(f"| æ€»æµ‹è¯•æ•° | {total_tests} |\n")
            f.write(f"| æˆåŠŸæŸ¥è¯¢ | {successful_tests} ({successful_tests/total_tests*100:.2f}%) |\n")
            f.write(f"| åŒ¹é…ç­”æ¡ˆ | {matched_tests} ({matched_tests/total_tests*100:.2f}%) |\n")
            f.write(f"| æ€»Tokenæ¶ˆè€— | {total_tokens:,} |\n")
            f.write(f"| å¹³å‡Token/é¢˜ | {avg_tokens:.0f} |\n")
            f.write(f"| æ€»è€—æ—¶ | {total_time:.1f}ç§’ |\n")
            f.write(f"| å¹³å‡è€—æ—¶/é¢˜ | {avg_time:.2f}ç§’ |\n\n")
            
            f.write("## ç±»å‹ç»Ÿè®¡\n\n")
            f.write(f"| ç±»å‹ | æ€»æ•° | æˆåŠŸ | åŒ¹é… | æˆåŠŸç‡ | åŒ¹é…ç‡ |\n")
            f.write(f"|------|------|------|------|--------|--------|\n")
            for t, stats in sorted(type_stats.items()):
                success_rate = f"{stats['success']/stats['total']*100:.1f}%" if stats['total'] > 0 else "N/A"
                match_rate = f"{stats['match']/stats['total']*100:.1f}%" if stats['total'] > 0 else "N/A"
                f.write(f"| {t} | {stats['total']} | {stats['success']} | {stats['match']} | {success_rate} | {match_rate} |\n")
            
            f.write("\n## è¯¦ç»†ç»“æœ\n\n")
            f.write("è¯·æŸ¥çœ‹ `benchmark_results.csv` æˆ– `benchmark_results.json` è·å–è¯¦ç»†ç»“æœã€‚\n")
        
        print(f"  âœ… æ‘˜è¦: {summary_file}")
        print(f"  âœ… æŠ¥å‘Š: {md_file}")
        
        # æ‰“å°æ‘˜è¦
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æµ‹è¯•æ‘˜è¦")
        print(f"{'='*80}")
        print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"   æˆåŠŸæŸ¥è¯¢: {successful_tests} ({successful_tests/total_tests*100:.2f}%)")
        print(f"   åŒ¹é…ç­”æ¡ˆ: {matched_tests} ({matched_tests/total_tests*100:.2f}%)")
        print(f"   æ€»Token: {total_tokens:,} (å¹³å‡ {avg_tokens:.0f}/é¢˜)")
        print(f"   æ€»è€—æ—¶: {total_time:.1f}ç§’ (å¹³å‡ {avg_time:.2f}ç§’/é¢˜)")
        print(f"{'='*80}\n")


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®è·¯å¾„
    baseline_dir = Path(__file__).parent.parent / "baseline"
    csv_path = baseline_dir / "æ•°æ®æº_é”€é‡.csv"
    benchmark_csv = Path(__file__).parent / "question" / "benchmark_100_questions_final.csv"
    
    # æ£€æŸ¥æ–‡ä»¶
    if not csv_path.exists():
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return
    
    if not benchmark_csv.exists():
        print(f"âŒ Benchmarkæ–‡ä»¶ä¸å­˜åœ¨: {benchmark_csv}")
        return
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = BenchmarkTester(str(csv_path))
    
    # è¯¢é—®æ˜¯å¦é™åˆ¶æµ‹è¯•æ•°é‡
    print(f"\nğŸ“‹ é…ç½®:")
    print(f"   æ•°æ®æº: {csv_path}")
    print(f"   æµ‹è¯•é›†: {benchmark_csv}")
    print(f"\næ˜¯å¦é™åˆ¶æµ‹è¯•æ•°é‡ï¼Ÿ")
    print(f"  - è¾“å…¥æ•°å­—: æµ‹è¯•å‰Nä¸ªé—®é¢˜")
    print(f"  - ç›´æ¥å›è½¦: æµ‹è¯•å…¨éƒ¨é—®é¢˜")
    
    max_tests = input("\nè¯·é€‰æ‹©: ").strip()
    max_tests = int(max_tests) if max_tests.isdigit() else None
    
    # è¿è¡Œbenchmark
    tester.run_benchmark(
        benchmark_csv=str(benchmark_csv),
        max_tests=max_tests
    )


if __name__ == "__main__":
    main()
